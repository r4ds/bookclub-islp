{
  "hash": "3e85649170309cf3a5e235e9d4c6c1b8",
  "result": {
    "markdown": "# Notes {-}\n\n## Questions to Answer\n\nRecall the `Advertising` data from **Chapter 2**. Here are a few important questions that we might seek to address:\n\n1. **Is there a relationship between advertising budget and sales?**\n2. **How strong is the relationship between advertising budget and sales?** Does knowledge of the advertising budget provide a lot of information about product sales?\n3. **Which media are associated with sales?**\n4. **How large is the association between each medium and sales?** For every dollar spent on advertising in a particular medium, by what amount will sales increase? \n5. **How accurately can we predict future sales?**\n6. **Is the relationship linear?** If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. If not, then it may still be possible to transform the predictor or the response so that linear regression can be used.\n7. **Is there synergy among the advertising media?** Or, in stats terms, is there an interaction effect?\n\n## Simple Linear Regression: Definition\n\n**Simple linear regression:** Very straightforward approach to predicting response $Y$ on predictor $X$.\n\n\n$$Y \\approx \\beta_{0} + \\beta_{1}X$$\n\n\n- Read \"$\\approx$\" as *\"is approximately modeled by.\"*\n- $\\beta_{0}$ = intercept\n- $\\beta_{1}$ = slope\n\n\n$$\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x$$\n\n\n- $\\hat{\\beta}_{0}$ = our approximation of intercept\n- $\\hat{\\beta}_{1}$ = our approximation of slope\n- $x$ = sample of $X$\n- $\\hat{y}$ = our prediction of $Y$ from $x$\n- hat symbol denotes \"estimated value\" \n\n- Linear regression is a simple approach to supervised learning\n\n## Simple Linear Regression: Visualization\n\n\n::: {.cell}\n::: {.cell-output-display}\n![For the `Advertising` data, the least squares fit for the regression of `sales` onto `TV` is shown. The fit is found by minimizing the residual sum of squares. Each grey line segment represents a residual. In this case a linear fit captures the essence of the relationship, although it overestimates the trend in the left of the plot.](images/fig3_1.jpg){width=100%}\n:::\n:::\n\n\n## Simple Linear Regression: Math\n\n- **RSS** = *residual sum of squares*\n\n\n$$\\mathrm{RSS} = e^{2}_{1} + e^{2}_{2} + \\ldots + e^{2}_{n}$$\n\n$$\\mathrm{RSS} = (y_{1} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{1})^{2} + (y_{2} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{2})^{2} + \\ldots + (y_{n} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{n})^{2}$$\n\n$$\\mathrm{RSS} = (y_{1} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{1})^{2} + (y_{2} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{2})^{2} + \\ldots + (y_{n} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{n})^{2}$$\n\n$$\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}$$\n\n\n- $\\bar{x}$, $\\bar{y}$ = sample means of $x$ and $y$\n\n### Visualization of Fit\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Contour and three-dimensional plots of the RSS on the `Advertising` data, using `sales` as the response and `TV` as the predictor. The red dots correspond to the least squares estimates $\\\\hat\\\\beta_0$ and $\\\\hat\\\\beta_1$, given by (3.4).](images/fig3_2.jpg){width=100%}\n:::\n:::\n\n\n**Learning Objectives:**\n\n- Perform linear regression with a **single predictor variable.**\n\n## Assessing Accuracy of Coefficient Estimates\n\n\n$$Y = \\beta_{0} + \\beta_{1}X + \\epsilon$$\n\n\n- **RSE** = *residual standard error*\n- Estimate of $\\sigma$\n\n\n$$\\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{n - 2}}$$\n\n$$\\mathrm{SE}(\\hat\\beta_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right],\\ \\ \\mathrm{SE}(\\hat\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$$\n\n\n- **95% confidence interval:** a range of values such that with 95% probability, the range will contain the true unknown value of the parameter\n  - If we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter\n\n\n$$\\hat\\beta_1 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_1)$$\n\n$$\\hat\\beta_0 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_0)$$\n\n\n**Learning Objectives:**\n\n- Estimate the **standard error** of regression coefficients.\n\n## Assessing the Accuracy of the Model\n\n- **RSE** can be considered a measure of the *lack of fit* of the model. \na\n- *$R^2$* statistic (also called coefficient of determination) provides an alternative that is in the form of a *proportion of the variance explained*, ranges from 0 to 1, a *good value* depends on the application.\n\n\n$$R^2 = 1 - \\frac{RSS}{TSS}$$\n\n\nwhere TSS is the *total sum of squarse*:\n\n$$TSS = \\Sigma (y_i - \\bar{y})^2$$\n\n\nQuiz: Can *$R^2$* be negative?\n\n[Answer](https://www.graphpad.com/support/faq/how-can-rsup2sup-be-negative/)\n\n## Multiple Linear Regression\n\n**Multiple linear regression** extends simple linear regression for *p* predictors:\n\n\n$$Y = \\beta_{0} + \\beta_{1}X_1 + \\beta_{2}X_2 + ... +\\beta_{p}X_p + \\epsilon_i$$\n\n\n- $\\beta_{j}$ is the *average* effect on $Y$ from $X_{j}$ holding all other predictors fixed.  \n\n- Fit is once again choosing the $\\beta_{j}$ that minimizes the RSS.\n\n- Example in book shows that although fitting *sales* against *newspaper* alone indicated a significant slope (0.055 +- 0.017), when you include *radio* in a multiple regression, *newspaper* no longer has any significant effect. (-0.001 +- 0.006) \n\n### Important Questions\n\n1. *Is at least one of the predictors $X_1$, $X_2$,  ... , $X_p$ useful in predicting\nthe response?*\n\n    F statistic close to 1 when there is no relationship, otherwise greater then 1.\n\n\n$$F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$$\n\n\n2. *Do all the predictors help to explain $Y$ , or is only a subset of the\npredictors useful?*\n\n   p-values can help identify important predictors, but it is possible to be mislead by this especially with large number of predictors. Variable selection methods include Forward selection, backward selection and mixed. Topic is continued in Chapter 6.\n\n3. *How well does the model fit the data?*\n\n    **$R^2$** still gives *proportion of the variance explained*, so look for values \"close\" to 1. Can also look at **RSE** which is generalized for multiple regression as:\n    \n\n$$RSE = \\sqrt{\\frac{1}{n-p-1}RSS}$$\n\n\n4. *Given a set of predictor values, what response value should we predict,\nand how accurate is our prediction?* \n\n    Three sets of uncertainty in predictions:\n    \n    * Uncertainty in the estimates of $\\beta_i$\n    * Model bias\n    * Irreducible error $\\epsilon$\n\n## Qualitative Predictors\n\n* Dummy variables: if there are $k$ levels, introduce $k-1$ dummy variables which are equal to one (\"one hot\") when the underlying qualitative predictor takes that value. For example if there are 3 levels, introduce two new dummy variables and fit the model:\n\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$$\n\n\n| Qualitative Predicitor | $x_{i1}$ | $x_{i2}$ |\n| ---------------------- |:--------:|:--------:|\n| level 0    (baseline)  |    0     |    0     |\n| level 1                |    1     |    0     |\n| level 2                |    0     |    1     |\n\n* Coefficients are interpreted the average effect relative to the baseline.\n\n* Alternative is to use index variables, a different coefficient for each level:\n\n\n$$y_i = \\beta_{0 1} + \\beta_{0 2} +\\beta_{0 3} + \\epsilon_i$$\n\n\n## Extensions\n\n- Interaction / Synergy effects\n    \n    Include a product term to account for synergy where one changes in one variable changes the association of the Y with another:\n    \n\n$$Y = \\beta_{0} + \\beta_{1}X_1 + \\beta_{2}X_2 +  \\beta_{3}X_1 X_2 + \\epsilon_i$$\n\n\n- Non-linear relationships (e.g. polynomial fits)\n\n\n$$Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}X^2 + ... \\beta_{n}X^n + \\epsilon_i$$\n\n\n## Potential Problems\n\n1. *Non-linear relationships* \n\n    Residual plots are useful tool to see if any remaining trends exist. If so consider fitting transformation of the data. \n    \n2. *Correlation of Error Terms*\n\n    Linear regression assumes that the error terms $\\epsilon_i$ are uncorrelated. Residuals may indicate that this is not correct (obvious *tracking* in the data). One could also look at the autocorrelation of the residuals. What to do about it?\n    \n3. *Non-constant variance of error terms*\n\n    Again this can be revealed by examining the residuals.  Consider transformation of the predictors to remove non-constant variance. The figure below shows residuals demonstrating non-constant variance, and shows this being mitigated to a great extent by log transforming the data.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Figure 3.11](images/fig3_11.png){width=100%}\n:::\n:::\n\n\n4. *Outliers*\n\n   - Outliers are points with for which $y_i$ is far from value predicted by the model (including irreducible error).  See point labeled '20' in figure 3.13.\n   - Detect outliers by plotting studentized residuals (residual $e_i$ divided by the estimated error) and look for residuals larger then 3 standard deviations in absolute value.\n   - An outlier may not effect the fit much but can have dramatic effect on the **RSE**. \n   - Often outliers are mistakes in data collection and can be removed, but could also be an indicator of a deficient model.  \n\n5. *High Leverage Points* \n\n   - These are points with unusual values of $x_i$.  Examples is point labeled '41' in figure 3.13.\n   - These points can have large impact on the fit, as in the example, including point 41 pulls slope up significantly.\n   - Use *leverage statistic* to identify high leverage points, which can be hard to identify in multiple regression.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Figure 3.13](images/fig3_13.png){width=100%}\n:::\n:::\n\n\n6. *Collinearity*\n\n   - Two or more predictor variables are closely related to one another.\n   - Simple collinearity can be identified by looking at correlations between predictors. \n   - Causes the standard error to grow (and p-values to grow)\n   - Often can be dealt with by removing one of the highly correlated predictors or combining them. \n   - *Multicollinearity* (involving 3 or more predictors) is not so easy to identify. Use *Variance inflation factor*, which is the ratio of the variance of $\\hat{\\beta_j}$ when fitting the full model to fitting the parameter on its own. Can be computed using the formula:\n    \n\n$$VIF(\\hat{\\beta_j}) = \\frac{1}{1-R^2_{X_j|X_{-j}}}$$\n\n\nwhere $R^2_{X_j|X_{-j}}$ is the $R^2$ from a regression of $X_j$ onto all the other predictors.\n\n## Answers to the Marketing Plan questions\n\n1. **Is there a relationship between advertising budget and sales?**\n\n    Tool: Multiple regression, look at F-statistic.\n\n2. **How strong is the relationship between advertising budget and sales?** \n\n    Tool: **$R^2$** and **RSE**\n    \n3. **Which media are associated with sales?**\n \n    Tool: p-values for each predictor's *t-statistic*.  Explored further in chapter 6.\n\n4. **How large is the association between each medium and sales?**\n\n    Tool: Confidence intervals on $\\hat{\\beta_j}$\n\n5. **How accurately can we predict future sales?**\n\n    Tool:: Prediction intervals for individual response, confidence intervals for average response.\n    \n    \n6. **Is the relationship linear?** \n\n    Tool: Residual Plots\n    \n7. **Is there synergy among the advertising media?** \n\n    Tool: Interaction terms and associated p-vales.\n\n## Comparison of Linear Regression with K-Nearest Neighbors\n\n- This section examines the K-nearest neighbor (KNN) method (a non-parameteric method).\n- This is essentially a k-point moving average.\n- This serves to illustrate the Bias-Variance trade-off nicely.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}