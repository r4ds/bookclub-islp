{
  "hash": "ee34acabde75dba55f5aa78d7199e10e",
  "result": {
    "markdown": "# Notes {-}\n\n## Introduction: Tree-based methods\n\n- Involve **stratifying** or **segmenting** the predictor space into a number of simple regions\n- Are simple and useful for interpretation\n- However, basic decision trees are NOT competitive with the best supervised learning approaches in terms of prediction accuracy\n- Thus, we also discuss **bagging**, **random forests**, and **boosting** (i.e., tree-based ensemble methods) to grow multiple trees which are then combined to yield a single consensus prediction\n- These can result in dramatic improvements in prediction accuracy (but some loss of interpretability)\n- Can be applied to both regression and classification\n\n## Regression Trees\n\nFirst, let's take a look at `Hitters` dataset.\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 322 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Names, League, Division, NewLeague\ndbl (17): AtBat, Hits, HmRun, Runs, RBI, Walks, Years, CAtBat, CHits, CHmRun...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 263 × 5\n   Names              Hits Years Salary log_Salary\n   <chr>             <dbl> <dbl>  <dbl>      <dbl>\n 1 -Alan Ashby          81    14  475         6.16\n 2 -Alvin Davis        130     3  480         6.17\n 3 -Andre Dawson       141    11  500         6.21\n 4 -Andres Galarraga    87     2   91.5       4.52\n 5 -Alfredo Griffin    169    11  750         6.62\n 6 -Al Newman           37     2   70         4.25\n 7 -Argenis Salazar     73     3  100         4.61\n 8 -Andres Thomas       81     2   75         4.32\n 9 -Andre Thornton      92    13 1100         7.00\n10 -Alan Trammell      159    10  517.        6.25\n# ℹ 253 more rows\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/08_1_salary_data.png){width=100%}\n:::\n\n::: {.cell-output-display}\n![](images/08_2_basic_tree.png){width=100%}\n:::\n:::\n\n\n- For the Hitters data, a regression tree for predicting the log salary of a baseball player based on:\n\n    1. number of years that he has played in the major leagues\n    2. number of hits that he made in the previous year\n\n## Terminology\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/08_3_basic_tree_term.png){width=100%}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![The three-region partition for the Hitters data set from the regression tree](images/08_4_hitters_predictor_space.png){width=100%}\n:::\n:::\n\n\n- Overall, the tree stratifies or segments the players into three regions of predictor space:\n  - R1 = {X \\| Years\\< 4.5}\n  - R2 = {X \\| Years\\>=4.5, Hits\\<117.5}\n  - R3 = {X \\| Years\\>=4.5, Hits\\>=117.5}\n  \n  where R1, R2, and R3 are **terminal nodes** (leaves) and green lines (where the predictor space is split) are the **internal nodes**\n\n- The number in each leaf/terminal node is the mean of the response for the observations that fall there\n\n## Interpretation of results: regression tree (Hitters data)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/08_2_basic_tree.png){width=100%}\n:::\n:::\n\n\n1. `Years` is the most important factor in determining `Salary`: players with less experience earn lower salaries than more experienced players\n2. Given that a player is less experienced, the number of `Hits` that he made in the previous year seems to play little role in his `Salary`\n3. But among players who have been in the major leagues for 5 or more years, the number of Hits made in the previous year does affect Salary: players who made more Hits last year tend to have higher salaries\n4. This is surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain\n\n## Tree-building process (regression)\n\n1. Divide the predictor space --- that is, the set of possible values for $X_1,X_2, . . . ,X_p$ --- into $J$ distinct and **non-overlapping** regions, $R_1,R_2, . . . ,R_J$\n - Regions can have ANY shape - they don't have to be boxes\n2. For every observation that falls into the region $R_j$, we make the same prediction: the **mean** of the response values in $R_j$\n3. The goal is to find regions (here boxes) $R_1, . . . ,R_J$ that **minimize** the $RSS$, given by\n\n\n$$\\mathrm{RSS}=\\sum_{j=1}^{J}\\sum_{i{\\in}R_j}^{}(y_i - \\hat{y}_{R_j})^2$$\n\n\nwhere $\\hat{y}_{R_j}$ is the **mean** response for the training observations within the $j$th box\n\n- Unfortunately, it is **computationally infeasible** to consider every possible partition of the feature space into $J$ boxes.\n\n## Recursive binary splitting\n\nSo, take a top-down, greedy approach known as recursive binary splitting:\n\n- **top-down** because it begins at the top of the tree and then successively splits the predictor space\n- **greedy** because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step\n\n1. First, select the predictor $X_j$ and the cutpoint $s$ such that splitting the predictor space into the regions ${\\{X|X_j<s\\}}$ and ${\\{X|X_j{\\ge}s}\\}$ leads to the greatest possible reduction in RSS\n2. Repeat the process looking for the best predictor and best cutpoint to split data further (i.e., split one of the 2 previously identified regions - not the entire predictor space) minimizing the RSS within each of the resulting regions\n3. Continue until a stopping criterion is reached, e.g., no region contains more than five observations\n4. Again, we predict the response for a given test observation using the **mean of the training observations** in the region to which that test observation belongs\n\nbut ...\n\n- The previous method may result in a tree that **overfits** the data. Why?\n- Tree is too leafy (complex)\n- A better strategy is to have a smaller tree with fewer splits, which will reduce variance and lead to better interpretation of results (at the cost of a little bias)\n- So we will prune\n\n## Pruning a tree\n\n1. Grow a very large tree $T_0$ as before\n2. Apply cost-complexity pruning to $T_0$ to obtain a sequence of BEST subtrees, as a function of $\\alpha$\n\nCost complexity pruning minimizes (Eq. 8.4)\n$\\sum_{m=1}^{|T|}\\sum_{x_i{\\in}R_m}(y_i-\\hat{y}_{R_m})^2 + \\alpha|T|$\n\nwhere\n\n$\\alpha$ $\\geq$ 0\n\n$|T|$ is the number of **terminal nodes** the sub tree $|T|$ holds\n\n$R_m$ is the rectangle/region (i.e., the subset of predictor space) corresponding to the $m$th terminal node\n\n$\\hat{y}_{R_m}$ is the **mean** response for the training observations in $R_m$\n\n- the tuning parameter $\\alpha$ controls:\n\n    a. a trade-off between the subtree's complexity (the number of terminal nodes)\n    b. the subtree's fit to the training data\n\n3. Choose $\\alpha$ using K-fold cross-validation\n\n    - repeat steps 1) and 2) for each $K-1/K$th fraction of training data\n    - average the results and pick $\\alpha$ to minimize the average MSE\n    - recall that in K-folds cross-validation (say K = 5): the model is estimated on 80% of the data five different times, the predictions are made for the remaining 20%, and the test MSEs are averaged\n\n4. Return to the subtree from Step 2) that corresponds to the chosen value of $\\alpha$\n\n## An example: tree pruning (Hitters dataset)\n\n- Results of fitting and pruning a regression tree on the Hitters data using 9 of the features\n- Randomly divided the data set in half (132 observations in training, 131 observations in the test set)\n- Built large regression tree on training data and varied $\\alpha$ in Eq. 8.4 to create subtrees with different numbers of terminal nodes\n- Finally, performed 6-fold cross-validation to estimate the cross-validated MSE of the trees as a function of $\\alpha$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/08_5_hitters_unpruned_tree.png){width=100%}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Training, cross-validation, and test MSE are shown as a function of the number of terminal nodes in the pruned tree. Standard error bands are displayed. The minimum cross-validation error occurs at a tree size of 3.](images/08_6_hitters_mse.png){width=100%}\n:::\n:::\n\n\n## Classification trees\n\n- Very similar to a regression tree except it predicts a qualitative (vs quantitative) response\n- We predict that each observation belongs to the **most commonly occurring class** of training observations in the region to which it belongs\n- In the classification setting, RSS cannot be used as a criterion for making the binary splits\n- A natural alternative to RSS is the classification **error rate**, i.e., the fraction of the training observations in that region that do not belong to the most common class:\n\n\n$$E = 1 - \\max_k(\\hat{p}_{mk})$$\n\n\nwhere $\\hat{p}_{mk}$ is the **proportion of training observations** in the $m$th region that are from the $k$th class\n\n- However, this error rate is unsuited for tree-based classification because $E$ does not change much as the tree grows (**lacks sensitivity**)\n- So, 2 other measures are preferable:\n\n    - The **Gini Index** defined by $$G = \\sum_{k=1}^{K}\\hat{p}_{mk}(1-\\hat{p}_{mk})$$ is a measure of total variance across the K classes\n    - The Gini index takes on a small value if all of the $\\hat{p}_{mk}$'s are close to 0 or 1\n    - For this reason the Gini index is referred to as a measure of node **purity** - a small value indicates that a node contains predominantly observations from a single class\n    - An alternative to the Gini index is **cross-entropy** given by\n  \n\n  $$D = - \\sum_{k=1}^{K}\\hat{p}_{mk}\\log(\\hat{p}_{mk})$$\n\n\n- The Gini index and cross-entropy are very similar numerically\n\n## Example: classification tree (Heart dataset)\n\n- Data contain a binary outcome HD (heart disease Y or N based on angiographic test) for 303 patients who presented with chest pain\n- 13 predictors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements\n- Cross-validation yields a tree with six terminal nodes\n\n::: {.cell}\n::: {.cell-output-display}\n![Heart data. Top: The unpruned tree. Bottom Left: Cross-validation error, training, and test error, for different sizes of the pruned tree. Bottom Right: The pruned tree corresponding to the minimal cross-validation error.](images/08_7_classif_tree_heart.png){width=100%}\n:::\n:::\n\n\n- **Comment**: Classification trees can be constructed if categorical PREDICTORS are present e.g., the first split: Thal is categorical (the 'a' in Thal:a indicates the first level of the predictor, i.e. Normal levels)\n- Additionally, notice that some of the splits yield two terminal nodes that have the same predicted value (see red box)\n- Regardless of the value of RestECG, a response value of *Yes* is predicted for those observations\n- Why is the split performed at all?\n  - Because it leads to increased node purity: all 9 of the observations corresponding to the right-hand leaf have a response value of *Yes*, whereas 7/11 of those corresponding to the left-hand leaf have a response value of *Yes*\n- Why is node purity important?\n  - Suppose that we have a test observation that belongs to the region given by that right-hand leaf. Then we can be pretty certain that its response value is *Yes*. In contrast, if a test observation belongs to the region given by the left-hand leaf, then its response value is **probably** *Yes*, but we are much less certain\n- Even though the split RestECG\\<1 does not reduce the classification error, it improves the Gini index and the entropy, which are more sensitive to node purity\n\n## Advantages/Disadvantages of decision trees\n\n- Trees can be displayed graphically and are **very easy to explain** to people\n- They mirror human decision-making\n- Can handle qualitative predictors without the need for dummy variables\n\nbut,\n\n- They do not have the same level of predictive accuracy\n- Can be very non-robust (i.e., a small change in the data can cause large change in the final estimated tree)\n- To improve performance, we can use an **ensemble** method, which combines many simple 'buidling blocks' (i.e., regression or classification trees) to obtain a single and potentially very powerful model\n- **ensemble** methods include: bagging, random forests, boosting, and Bayesian additive regression trees\n\n## Bagging\n\n- Also known as **bootstrap aggregation** is a general-purpose procedure for reducing the variance of a statistical learning method\n- It's useful and frequently used in the context of decision trees\n- Recall that given a set of $n$ independent observations $Z_1,..., Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar{Z}$ of the observations is given by $\\sigma^2/n$\n- So, **averaging a set of observations** reduces variance\n- But, this is not practical because we generally do not have access to multiple training sets!\n- What can we do?\n\n- Cue the bootstrap, i.e., take repeated samples from the single training set\n- Generate $B$ different bootstrapped training data set\n- Then train our method on the $b$th bootstrapped training set to get $\\hat{f}^{*b}$, the prediction at a point x\n- Average all the predictions to obtain $$\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)$$\n- In the case of classification trees:\n  - for each test observation:\n    - record the class predicted by each of the $B$ trees\n    - take a **majority vote**: the overall prediction is the most commonly occurring class among the $B$ predictions\n\n**Comment**: The number of trees $B$ is not a critical parameter with bagging - a large $B$ will not lead to overfitting\n\n## Out-of-bag error estimation\n\n- But how do we estimate the test error of a bagged model?\n- It's pretty straightforward:\n  1. Because trees are repeatedly fit to bootstrapped subsets of observations, on average each bagged tree uses about 2/3 of the observations\n  2. The leftover 1/3 not used to fit a given bagged tree are called **out-of-bag** (OOB) observations\n  3. We can predict the response for the $i$th observation using each of the trees in which that observation was OOB. Gives around B/3 predictions for the $i$th observation (which we then average)\n  4. This estimate is essentially the LOO cross-validation error for bagging (if $B$ is large)\n\n## Variable importance measures\n\n- Bagging results in improved accuracy over prediction using a single tre\n- But, it can be difficult to interpret the resulting model:\n  - we can't represent the statistical learning procedure using a single tree\n  - it's not clear which variables are most important to the procedure (i.e., we have many trees each of which may give a differing view on the importance of a given predictor)\n- So, which predictors are important?\n  - An overall summary of the importance of each predictor can be achieved by recording how much the average $RSS$ or Gini index **improves (or decreases)** when each tree is split over a given predictor (averaged over all $B$ trees)\n    - a large value = important predictor\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A variable importance plot for the Heart data. Variable importance is computed using the mean decrease in Gini index, and expressed relative to the maximum.](images/08_8_var_importance.png){width=100%}\n:::\n:::\n\n\n## Random forests\n\n- A problem with bagging is that bagged trees may be **highly similar** to each other.\n- For example, if there is a strong predictor in the data set, most of the bagged trees will **use this strong predictor** in the top split so that\n  - the trees will look quite similar\n  - predictions from the bagged trees will be highly correlated\n- Averaging many highly correlated quantities does not lead to as large a reduction in variance as averaging many uncorrelated quantities\n\n## Random forests: advantages over bagging\n\n- Random forests overcome this problem by forcing each split to consider only a **subset** of the predictors (typically a random sample $m \\approx \\sqrt{p}$)\n- Thus at each split, the algorithm is NOT ALLOWED to consider a majority of the available predictors (essentially $(p - m)/p$ of the splits will not even consider the strong predictor, giving other predictors a chance)\n- This *decorrelates* the trees and makes the average of the resulting trees less variable (more reliable)\n- Only difference between bagging and random forests is the choice of predictor subset size $m$ at each split: if a random forest is built using $m = p$ that's just bagging\n- For both, we build a number of decision trees on bootstrapped training samples\n\n## Example: Random forests versus bagging (gene expression data)\n\n- High-dimensional biological data set: contains gene expression measurements of 4,718 genes measured on tissue samples from 349 patients\n- Each of the patient samples has a qualitative label with 15 different levels: *Normal* or one of 14 different cancer types\n- Want to predict cancer type based on the 500 genes that have the largest variance in the training set\n- Randomly divided the observations into training/test and applied random forests (or bagging) to the training set for 3 different values of $m$ (the number of predictors available at each split)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Results from random forests for the 15-class gene expression data set with p = 500 predictors. The test error is displayed as a function of the number of trees. Random forests (m < p) lead to a slight improvement over bagging (m = p). A single classification tree has an error rate of 45.7%.](images/08_9_rand_forest_gene_exp.png){width=100%}\n:::\n:::\n\n\n## Boosting\n\n- Yet another approach to improve prediction accuracy from a decision tree\n- Can also be applied to many statistical learning methods for regression or classification\n- Recall that in bagging each tree is built on a bootstrap training data set\n- In boosting, each tree is grown sequentially using information from previously grown trees:\n  - given the current model, we fit a decision tree to the residuals of the model (rather than the outcome *Y*) as the response\n  - we then add this new decision tree into the fitted function (model) in order to update the residuals\n  - Why? this way each tree is built on information that the previous trees were unable to 'catch'\n\n## Boosting algorithm\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/08_10_boosting_algorithm.png){width=100%}\n:::\n:::\n\n\nwhere:\n\n$\\hat{f}(x)$ is the decision tree (model)\n\n$r$ = residuals\n\n$d$ = number of splits in each tree (controls the complexity of the boosted ensemble)\n\n$\\lambda$ = shrinkage parameter (a small positive number that controls the rate at which boosting learns; typically 0.01 or 0.001 but right choice can depend on the problem)\n\n- Each of the trees can be small, with just a few terminal nodes (determined by $d$)\n- By fitting small trees to the residuals, we slowly improve our model ($\\hat{f}$) in areas where it doesn't perform well\n- The shrinkage parameter $\\lambda$ slows the process down further, allowing more and different shaped trees to 'attack' the residuals\n- Unlike bagging and random forests, boosting can OVERFIT if $B$ is too large. $B$ is selected via cross-validation\n\n## Example: Boosting versus random forests\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Results from performing boosting and random forests on the 15-class gene expression data set in order to predict cancer versus normal. The test error is displayed as a function of the number of trees. For the two boosted models, lambda = 0.01. Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant. The test error rate for a single tree is 24 %.](images/08_11_boosting_gene_exp_data.png){width=100%}\n:::\n:::\n\n\n- Notice that because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient in boosting (versus random forests)\n- Random forests and boosting are among the state-of-the-art methods for supervised learning (but, their results can be difficult to interpret)\n\n## Bayesian additive regression trees (BART)\n\n- Recall that in bagging and random forests, each tree is built on a **random sample of data and/or predictors** and each tree is built **independently** of the others\n- BART is related to both - what is new is HOW the new trees are generated\n- **NOTE**: only BART for regression is described in the book\n\n## BART notation\n\n- Let $K$ be the total **number of regression trees** and\n- $B$ be the **number of iterations** the BART algorithm will run for\n- Let $\\hat{f}^b_k(x)$ be the **prediction** at $x$ for the $k$th regression tree used in the $b$th iteration of the BART algorithm\n- At the end of each iteration, the $K$ trees from that iteration will be summed:\n\n\n$$\\hat{f}^b(x) = \\sum_{k=1}^{K}\\hat{f}^b_k(x)$$ for $b=1,...,B$\n\n## BART algorithm\n\n- In the first iteration of the BART algorithm, all $K$ trees are initialized to have 1 root node, with $\\hat{f}^1_k(x) = \\frac{1}{nK}\\sum_{i=1}^{n}y_i$\n  - i.e., the mean of the response values divided by the total number of trees\n- Thus, for the first iteration ($b = 1$), the prediction for all $K$ trees is just the mean of the response\n\n$\\hat{f}^1(x) = \\sum_{k=1}^K\\hat{f}^1_k(x) = \\sum_{k=1}^K\\frac{1}{nK}\\sum_{i=1}^{n}y_i = \\frac{1}{n}\\sum_{i=1}^{n}y_i$\n\n## BART algorithm: iteration 2 and on\n\n- In subsequent iterations, BART updates each of the $K$ trees one at a time\n- In the $b$th iteration to update the $k$th tree, we subtract from each response value the predictions from all but the $k$th tree, to obtain a partial residual:\n\n$r_i = y_i - \\sum_{k'<k}\\hat{f}^b_{k'}(x_i) - \\sum_{k'>k}\\hat{f}^{b-1}_{k'}(x_i)$\n\nfor the $i$th observation, $i = 1, …, n$\n\n- Rather than fitting a new tree to this partial residual, BART chooses a perturbation to the tree from a previous iteration $\\hat{f}^{b-1}_{k}$ favoring perturbations that improve the fit to the partial residual\n- To perturb trees:\n  - change the structure of the tree by adding/pruning branches\n  - change the prediction in each terminal node of the tree\n- The output of BART is a collection of prediction models:\n\n$\\hat{f}^b(x) = \\sum_{k=1}^{K}\\hat{f}^b_k(x)$\n\nfor $b = 1, 2,…, B$\n\n## BART algorithm: figure\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/08_12_bart_algorithm.png){width=100%}\n:::\n:::\n\n- **Comment**: the first few prediction models obtained in the earlier iterations (known as the $burn-in$ period; denoted by $L$) are typically thrown away since they tend to not provide very good results, like you throw away the first pancake of the batch\n\n## BART: additional details\n\n- A key element of BART is that a fresh tree is NOT fit to the current partial residual: instead, we improve the fit to the current partial residual by slightly modifying the tree obtained in the previous iteration (Step 3(a)ii)\n- This guards against overfitting since it limits how \"hard\" the data is fit in each iteration\n- Additionally, the individual trees are typically pretty small\n- BART, as the name suggests, can be viewed as a *Bayesian* approach to fitting an ensemble of trees:\n  - each time a tree is randomly perturbed to fit the residuals = drawing a new tree from a *posterior* distribution\n\n## To apply BART:\n\n- We must select the number of trees $K$, the number of iterations $B$ and the number of burn-in iterations $L$\n- Typically, large values are chosen for $B$ and $K$ and a moderate value for $L$: e.g. $K$ = 200, $B$ = 1,000 and $L$ = 100\n- BART has been shown to have impressive out-of-box performance - i.e., it performs well with minimal tuning\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}