[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Introduction to Statistical Learning using Python",
    "section": "Welcome",
    "text": "Welcome\nThis is a companion for the book Introduction to Statistical Learning with Python.\nThis website is being developed by the R4DS Online Learning Community. Follow along and join the community to participate.\nThis companion follows the R4DS Online Learning Community Code of Conduct."
  },
  {
    "objectID": "index.html#book-club-meetings",
    "href": "index.html#book-club-meetings",
    "title": "Introduction to Statistical Learning using Python",
    "section": "Book club meetings",
    "text": "Book club meetings\n\nEach week, a volunteer will present a chapter from the book.\n\nThis is the best way to learn the material.\n\nPresentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter.\nMore information about how to present is available in the GitHub repo.\nPresentations will be recorded and will be available on the R4DS Online Learning Community YouTube Channel."
  },
  {
    "objectID": "01_main.html#learning-objectives",
    "href": "01_main.html#learning-objectives",
    "title": "1. Introduction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nRecognize various types of statistical learning.\nUnderstand why this book is useful for you.\nBe able to read mathematical notation used throughout this book.\nDescribe the overall layout of this book.\nBe able to find data used in examples throughout the book."
  },
  {
    "objectID": "01_notes.html#what-is-statistical-learning",
    "href": "01_notes.html#what-is-statistical-learning",
    "title": "Notes",
    "section": "What is statistical learning?",
    "text": "What is statistical learning?\nStatistical learning is the theoretical foundation for machine learning framework. It makes connections between the fields of statistics, linear algebra and functional analysis.\nIn particular, the Statistical learning theory deals with the problem of finding a predictive function based on data and this is what is best known as supervised learning, in this book we will see more than just theory, as we will deal with unsupervised learning as well as making practical applications.\n\nSupervised: “Building a model to predict an output from inputs.”\n\nPredict wage from age, education, and year.\nPredict market direction from previous days' performance.\n\nUnsupervised: Inputs but no specific outputs, find relationships and structure.\n\nIdentify clusters within cancer cell lines."
  },
  {
    "objectID": "01_notes.html#why-islp",
    "href": "01_notes.html#why-islp",
    "title": "Notes",
    "section": "Why ISLP?",
    "text": "Why ISLP?\n\n“Facilitate the transition of statistical learning from an academic to a mainstream field.”\nMachine learning* is useful to everyone, let’s all learn enough to use it responsibly.\nPython “labs” make this make sense for this community!"
  },
  {
    "objectID": "01_notes.html#premises-of-islp",
    "href": "01_notes.html#premises-of-islp",
    "title": "Notes",
    "section": "Premises of ISLP",
    "text": "Premises of ISLP\nFrom Page 9 of the Introduction:\n\n“Many statistical learning methods are relevant and useful in a wide range of academic and non-academic disciplines, beyond just the statistical sciences.”\n“Statistical learning should not be viewed as a series of black boxes.”\n“While it is important to know what job is performed by each cog, it is not necessary to have the skills to construct the machine inside the box!”\n“We presume that the reader is interested in applying statistical learning methods to real-world problems.”"
  },
  {
    "objectID": "01_notes.html#notation",
    "href": "01_notes.html#notation",
    "title": "Notes",
    "section": "Notation",
    "text": "Notation\n\nn = number of observations (rows)\np = number of features/variables (columns)\nWe’ll come back here if we need to as we go!\nSome symbols they assume we know:\n\n\\(\\in\\) = “is an element of”, “in”\n\\({\\rm I\\!R}\\) = “real numbers”"
  },
  {
    "objectID": "01_notes.html#what-have-we-gotten-ourselves-into",
    "href": "01_notes.html#what-have-we-gotten-ourselves-into",
    "title": "Notes",
    "section": "What have we gotten ourselves into?",
    "text": "What have we gotten ourselves into?\nAn Introduction to Statistical Learning (ISL by James, Witten, Hastie and Tibshiraniis), is a collection of modern statistical methods for modeling and making predictions from real-world data.\nIt is a middle way between theoretical statistics and the practice of applying statistics to real-world problems.\nIt can be considered as a user manual, with self-contained Python labs, which lead you through the use of different methods for applying statistical analysis to different kinds of data.\n\n2: Terminology & main concepts\n3-4: Classic linear methods\n5: Resampling (so we can choose the best method)\n6: Modern updates to linear methods\n7+: Beyond Linearity (we can worry about details as we get there)"
  },
  {
    "objectID": "01_notes.html#wheres-the-data",
    "href": "01_notes.html#wheres-the-data",
    "title": "Notes",
    "section": "Where’s the data?",
    "text": "Where’s the data?\npip install ISLP\nWe’ll look at this data in more detail below."
  },
  {
    "objectID": "01_notes.html#some-useful-resources",
    "href": "01_notes.html#some-useful-resources",
    "title": "Notes",
    "section": "Some useful resources:",
    "text": "Some useful resources:\n\nthe book page: statlearning.com\npdf of the book: ISLRv2_website\nISLP labs\ncourse on edX: statistical-learning\nyoutube channel: playlists\nexercise solutions: applied solutions\n\nSome more theoretical resources:\n\nThe Elements of Statistical Learning (ESL, by Hastie, Tibshirani, and Friedman) ESLII"
  },
  {
    "objectID": "01_notes.html#what-is-covered-in-the-book",
    "href": "01_notes.html#what-is-covered-in-the-book",
    "title": "Notes",
    "section": "What is covered in the book?",
    "text": "What is covered in the book?\nThe book provides a series of toolkits classified as supervised or unsupervised techniques for understanding data.\nThe second edition of the book (2021) contains additions within the most updated statistical analysis.\n\n\n\n\n\nEditions"
  },
  {
    "objectID": "01_notes.html#how-is-the-book-divided",
    "href": "01_notes.html#how-is-the-book-divided",
    "title": "Notes",
    "section": "How is the book divided?",
    "text": "How is the book divided?\nThe book is divided into 13 chapters covering:\n\nIntroduction and Statistical Learning:\n\nSupervised Versus Unsupervised Learning\nRegression Versus Classification Problems\n\n\nLinear statistical learning\n\nLinear Regression:\n\nbasic concepts\nintroduction of K-nearest neighbor classifier\n\nClassification:\n\nlogistic regression\nlinear discriminant analysis\n\nResampling Methods:\n\ncross-validation\nthe bootstrap\n\nLinear Model Selection and Regularization: potential improvements over standard linear regression\n\nstepwise selection\nridge regression\nprincipal components regression\nthe lasso\n\n\nNon-linear statistical learning\n\nMoving Beyond Linearity:\n\nPolynomial Regression\nRegression Spline\nSmoothing Splines\nLocal Regression\nGeneralized Additive Models\n\nTree-Based Methods:\n\nDecision Trees\nBagging, Random Forests, Boosting, and Bayesian Additive Regression Trees\n\nSupport Vector Machines (linear and non-linear classification)\nDeep Learning (non-linear regression and classification)\nSurvival Analysis and Censored Data\nUnsupervised Learning:\n\nPrincipal components analysis\nK-means clustering\nHierarchical clustering\n\nMultiple Testing\n\nEach chapter includes 1 self-contained R lab on the topic"
  },
  {
    "objectID": "01_notes.html#some-examples-of-the-problems-addressed-with-statistical-analysis",
    "href": "01_notes.html#some-examples-of-the-problems-addressed-with-statistical-analysis",
    "title": "Notes",
    "section": "Some examples of the problems addressed with statistical analysis",
    "text": "Some examples of the problems addressed with statistical analysis\n\nIdentify the risk factors for some type of cancers\nPredict whether someone will have a hearth attack on the basis of demographic, diet, and clinical measurements\nEmail spam detection\nClassify a tissue sample into one of several cancer classes, based on a gene expression profile\nEstablish the relationship between salary and demographic variables in population survey data\n\n(source)"
  },
  {
    "objectID": "01_notes.html#datasets-provided-in-the-islp-package",
    "href": "01_notes.html#datasets-provided-in-the-islp-package",
    "title": "Notes",
    "section": "Datasets provided in the ISLP package",
    "text": "Datasets provided in the ISLP package\nThe book provides the ISLP Python package with all the datasets needed the analysis.\n\n\n\n\n\nDatasets in ISLP package"
  },
  {
    "objectID": "01_video.html#cohort-01",
    "href": "01_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\n00:07:02    Lucio Cornejo:    start\n00:12:04    Mateo Vega:    Hey everyone! sorry I cannot turn on the mic right now\n00:12:32    Mateo Vega:    On the ISLR?\n00:12:50    Mateo Vega:    No hahaha\n00:13:48    Mateo Vega:    Yes, both things would be fine\n00:13:56    Shah Nawaz:    both\n00:19:20    Mateo Vega:    I would like to use quarto\n00:19:52    Mateo Vega:    I already have some practice with it so no problem\n00:21:38    Lydia Gibson:    https://quarto.org/docs/presentations/\n00:22:16    Lydia Gibson:    Quarto book sounds good\n00:37:56    Lydia Gibson:    I think that's classification\n00:41:34    Mateo Vega:    See you, thanks!\n00:41:40    Lucio Cornejo:    end"
  },
  {
    "objectID": "02_main.html#learning-objectives",
    "href": "02_main.html#learning-objectives",
    "title": "2. Statistical Learning",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nCompare supervised vs unsupervised learning.\nCompare regression vs classification problems.\nMeasure the accuracy of regression model fits.\nMeasure the accuracy of classification model fits.\nDescribe the trade-off between more accurate models and more interpretable models.\nCompare Parametric vs Nonparametric models\nUnderstand overfitting.\nDescribe how bias and variance contribute to the model error."
  },
  {
    "objectID": "02_notes.html#what-is-statistical-learning",
    "href": "02_notes.html#what-is-statistical-learning",
    "title": "Notes",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nIn this chapter will deal with developing an accurate model that can be used to predict some value.\nNotation:\n\nInput variables: \\(X_1, \\cdots, X_p\\)\nAlso known as predictors, features, independent variables.\nOutput variable: \\(Y\\)\nAlso known as response or dependent variable.\n\nWe assume there is some relationship between \\(Y\\) and \\(X = \\left( X_1, \\cdots, X_p \\right)\\), which we write as:\n\\[Y = f(X) + \\epsilon\\]\n, where \\(\\epsilon\\) is a random error term which is independent from \\(X\\) and has mean zero; and, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y\\) .\n\n\n\n\n\nIncome data set\n\n\n\n\nIn essence, statistical learning deals with different approaches to estimate \\(f\\) .\n\nWhy estimate \\(f\\)?\nTwo main reasons to estimate \\(f\\):\n\nPrediction\n\nPredict \\(Y\\) using a set of inputs \\(X\\) .\nRepresentation: \\(\\hat{Y}= \\hat{f}(X)\\), where \\(\\hat{f}\\) represents our estimate for \\(f\\), and \\(\\hat{Y}\\) our prediction for \\(Y\\) .*\nIn this setting, \\(\\hat{f}\\) is often treated as a black-box, meaning we don’t mind not knowing the exact form of \\(\\hat{f}\\), if it generates accurate predictions for \\(Y\\) .\n\\(\\hat{Y}\\)’s accuracy depends on:\n\nReducible error\n\nDue to \\(\\hat{f}\\) not being a perfect estimate for \\(f\\).\nCan be reduced by using a proper statistical learning technique.\n\nIrreducible error\n\nDue to \\(\\epsilon\\) and its variability.\n\\(\\epsilon\\) is independent from \\(X\\), so no matter how well we estimate \\(f\\), we can’t reduce this error.\n\n\nThe quantity \\(\\epsilon\\) may contain unmeasured variables useful for predicting \\(Y\\); or, may contain unmeasure variation, so no prediction model will be perfect.\nMathematical form, after choosing predictors \\(X\\) and an estimate \\(\\hat{f}\\):\n\n\\[\nE( Y - \\hat{Y} )^2 =\nE(f(X) + \\epsilon - \\hat{f}(X))^2 =\n\\underbrace{[f(X) - \\hat{f}(X)]^2}_{reducible} +\n\\underbrace{\\text{ Var}(\\epsilon)}_{irreducible}\\; .\n\\]\nIn practice, we almost always don’t know how \\(\\epsilon\\)’s variability affects our model, so, in this boook, we will focus on techniques for estimating \\(f\\) .\n\n\nInference\nIn this case, we are interested in understanding the association between \\(Y\\) and \\(X_1, \\cdots, X_p\\).\n\nFor example:\n\nWhich predictors are most associated with response?\nWhat is the relationship between the response and each predictor?\nCan such relationship be summarized via a linear equation, or is it more complex?\n\n\nThe exact form of \\(\\hat{f}\\) is required.\nLinear models allow for easier interpretability, but can lack in prediction accuracy; while, non-linear models can be more accurate, but less interpretable.\n\n\n\nHow do we estimate \\(f\\) ?\n\nFirst, let’s agree on some conventions:\n\n\\(n\\) : Number of observations.\n\\(x_{ij}\\): Value of the \\(j\\text{th}\\) predictor, for \\(i\\text{th}\\) observation.\n\\(y_i\\) : Response variable for \\(i\\text{th}\\) observation.\nTraining data:\n\nSet of observations.\nUsed to esmitate \\(f\\).\n\\(\\left\\{ (x_1, y_1), \\cdots, (x_n, y_n) \\right\\}\\), where \\(x_i = (x_{i1}, \\cdots, x_{ip})^T\\) .\n\n\nGoal: Find a function \\(\\hat{f}\\) such that \\(Y\\approx\\hat{f}(X)\\) for any observation \\((X,Y)\\) .\nMost statistical methods for achieving this goal can be characterized as either parametric or non-parametric.\n\n\nParametric methods\n\nSteps:\n\nMake an assumption about the form of \\(f\\).\nIt could be linear (\\(f(X) = \\beta_0 + \\beta_1 X_1 + \\cdot + \\beta_p X_p,\\) parameters \\(\\beta_0, \\cdots, \\beta_p\\) to be estimated) or not.\nThe model has been selected.\nNow, we need a procedure to fit the model using the training data.\nThe most common of such fitting procedures is called (ordinary) least squares.\n\nVia these steps, the problem of estimating \\(f\\) has been reduced to a problem of estimating a set of parameters.\nWe can make the models more flexible via considering a greater number of parameters, but, this can lead to overfitting the data, that is, following the errors/noise too closely, which will not yield accurate estimates of the response for observations outside of the original training data.\n\n\n\n\nNon-parametric methods\n\nNo assumptions about the form of \\(f\\) are made.\nInstead, we seek an estimate of \\(f\\) which that gets as close to the data point as possible.\nHas the potential to fit a wider range of possible forms for \\(f\\).\nTipically requires a very large number of observations (compared to paramatric approach) in order to accurately estimate \\(f\\).\n\n\n\n\nThe trade-off between prediction accuracy and model interpretability\nWe’ve seen that parametric models are usually restrictive; and, non-parametric models, flexible. However:\n\nRestrictive models are usually more interpretable, so they are useful for inference.\nFlexible models can be difficult to interpret, due to the complexity of \\(\\hat{f}\\).\n\nDespite this, we will often obtain more accurate predictions usinf a less flexible method, due to the potential for overfitting the data in highly flexible models.\n\n\nSupervised vs Unsupervised Learning\nIn supervised learning, we wish to fit a model that relates inputs/predictors to some output.\nIn unsupervised learning, we lack a reponse/variable to predict. Instead, we seek to understand the relationships between the variables or between the observations.\nThere are instances where a mix of such methods is required (semi-supervised learning problems), but such topic will not be covered in this book.\n\n\nRegression vs Classification problems\n\nIf the response is …\n\nQuantitative, then, it’s a regression problem.\nCategorical, then, it’s a classification problem.\n\nMost of the methods covered in this book can be applied regardless of the predictor variable type, but the categorical variables will require some pre-processing."
  },
  {
    "objectID": "02_notes.html#assessing-model-accuracy",
    "href": "02_notes.html#assessing-model-accuracy",
    "title": "Notes",
    "section": "Assessing model accuracy",
    "text": "Assessing model accuracy\n\nThere is no best method for Statistical Learning, the method’s efficacy can depend on the data set.\nFor a specific data set, how do we select the best Statistics approach?\n\n\nMeasuring the quality of fit\n\nThe performance of a statistical learning method can be evaluated comparing the predictions of the model, with their true/real response.\nMost commonly used measure for this:\n\nMean squared error\n\\(\\text{ MSE } = \\dfrac{1}{n}\\displaystyle{ \\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2 }\\)\nSmall MSE means that the predicted and the true responses are very close.\n\nWe want the model to accurately predict unseen data (testing data), not so much the training data, where the response is already known.\nThe best model will be the one which produces the lowest test MSE, not the lowest training MSE.\nIt’s not true that the model with lowest training MSE will also have the lowest test MSE.\n\n\n\n\n\n\nTraining MSE vs Test MSE\n\n\n\n\n\nFundamental property: For any data set and any statistical learning method used, as the flexibility of the statistical learning method increases:\n\nThe training MSE decreases monotonically.\nThe test MSE graph has a U-shape.\n\n\n\nAs model flexibility increases, training MSE will decrease, but the test MSE may not.\n\n\nSmall training MSE but big test MSE implies having overfitted the data.\nRegardless of overfitting or not, we almost always expect \\(\\text{training MSE } &lt; \\text{ testing MSE }\\), beacuse most statistical learning methods seek to minimize the training MSE.\nEstimating test MSE is very difficult, usually because lack of data. Later in this book, we’ll discuss approaches to estimate the mininum point for the test MSE curve.\n\n\n\nThe Bias-Variance Trade-off\n\nDefinition: The expected test MSE at \\(x_0\\) (\\(E(y_0 - \\hat{f}(x_0))^2\\)) refers to the averga test MSE that we would obtain after repeatedly estimating \\(f\\) using a large number of training sets, and tested each esimate at \\(x_0\\).\nDefinition: The variance of a statistical learning method which produces an estimate \\(\\hat{f}\\) refers to how the estimate function changes, for different training sets.\nDefinition: Bias refers to the error generated by approximating a possibly complicated model (like in real-life usually), by a much simpler one … (how \\(f\\) and the possibles \\(\\hat{f}\\) differ).\nAs a general rule, the more flexible a statistical method, the higher its variance and lower its bias.\nFor any given value \\(x_0\\), the following can be proved:\n\n\\[\nE(y_0 - \\hat{f}(x_0))^2 = \\text{Var}(\\hat{f}(x_0)) + \\text{Bias}(\\hat{f}(x_0))^2 + \\text{ Var }(\\epsilon)\n\\]\n\nDue to variance and squared bias being non negative, the previous equation implies that, to minimize the expected test error, we require a statistical learnig method which achieves low variance and low bias.\nThe tradeoff:\n\nExtremely low bias but high variance: For example, draw a line which passes over every single point in the training data.\nExtremely low variance but high bias: For example, fit a horizontal line to the data.\n\nThe challenge lies in finding a method for which both the variance and the squared bias are low.\n\n\nIn a real-life situation, \\(f\\) is usually unkwon, so it’s not possible to explicitly compute the test MSE, bias or variance of a statistical method.\nThe test MSE can be estimated using cross-validation, but we’ll discuss it later in this book.\n\n\n\nThe Classification setting\nLet’s see how the concepts recently discussed change when we the prediction is a categorical variable.\nThe most common approach for quantifying the accuracy of our estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes made by applying \\(\\hat{f}\\) to the training observations:\n\\[\n\\dfrac{1}{n}\\displaystyle{ \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)}\n\\]\n, where \\(I\\) is \\(1\\) when \\(y_i = \\hat{y}_i\\), and \\(0\\) otherwise.\n\nThe test error rate is defined as \\(\\text{ Average}(I(y_i \\neq \\hat{y}_i))\\), where the average is computed by comparing the predictions \\(\\hat{y}_i\\) with the true response \\(y_i\\).\nA good classifier is one for which the test error is smallest."
  },
  {
    "objectID": "02_video.html#cohort-01",
    "href": "02_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\n00:09:36    Isabella Velásquez: https://github.com/r4ds/bookclub-py4da\n00:15:43    Layla Bouzoubaa:    heart heart heart\n00:15:50    Layla Bouzoubaa:    I am lving quarto\n00:16:07    Layla Bouzoubaa:    😻😻😻😻\n00:16:27    shamsuddeen:    Yes\n00:17:49    Jadey Ryan: thank you so much Isabella! so helpful 🥳\n00:21:12    Isabella Velásquez: Get started with Quarto: https://quarto.org/docs/get-started/\n00:21:50    Ron:    Busted ;)\n00:39:01    Isabella Velásquez: import pandas as pd\n\n%pdef pd.read_csv\n00:39:23    Isabella Velásquez: ^ this runs (no parentheses)\n00:41:15    Ron:    For simpler functions %pdef is pretty helpful, for example %pdef pd.DataFrame is only a few lines ;)\n00:41:46    Layla Bouzoubaa:    ahh, perfect! thank you much!\n00:43:13    Layla Bouzoubaa:    wooaahh\n00:52:53    Isabella Velásquez: Error in \"5\" + 6 : non-numeric argument to binary operator\n01:02:27    Ron:    I LOVE f-strings.\n01:06:45    Isabella Velásquez: I have to go, thanks so much, this was great!\n01:07:19    Layla Bouzoubaa:    me as well, have a great weekend all!\n01:08:17    Jim Gruman: thankyou!!!\n01:08:25    Jadey Ryan: thank you!!!"
  },
  {
    "objectID": "how-to.html#set-up-quarto",
    "href": "how-to.html#set-up-quarto",
    "title": "How to add to the book",
    "section": "Set up Quarto",
    "text": "Set up Quarto\nThis book is made with Quarto. Please see the Get Started chapter of the Quarto documentation to learn how to install and run Quarto in your IDE."
  },
  {
    "objectID": "how-to.html#add-to-book",
    "href": "how-to.html#add-to-book",
    "title": "How to add to the book",
    "section": "Add to book",
    "text": "Add to book\nOnce you have everything set up, forked the repo, and cloned to your computer, you can add a new chapter to the book.\nCreate a new file in the repository folder. For example, to create a new file called 01_exercises.qmd, navigate to the folder then create one using touch 01_exercises.qmd. If you are using VSCode, you can use the Quarto plug-in. You can use plain .md files, Quarto .qmd, or Jupyter .ipynb files in this book. Check out the files under Examples to see the various options.\nWrite in what you would like in the file.\nThen, in the _quarto.yml file, under chapters, add a part with your chapter. The file listed after part is the first page of chapter; the ones under chapters will be subpages.\n  - part: 01_main.qmd\n      chapters: \n      - 01_notes.qmd\n      - 01_video.qmd\n      - 01_exercises.qmd"
  },
  {
    "objectID": "how-to.html#render-the-book",
    "href": "how-to.html#render-the-book",
    "title": "How to add to the book",
    "section": "Render the book",
    "text": "Render the book\nOnce you have added and edited your files, don’t forget to render the book. Run this in the terminal:\nquarto render --to html"
  },
  {
    "objectID": "how-to.html#push-up-to-github",
    "href": "how-to.html#push-up-to-github",
    "title": "How to add to the book",
    "section": "Push up to GitHub",
    "text": "Push up to GitHub\nPush your changes to your forked repo and then create a pull request for the R4DS admins to merge your changes.\ngit add .\ngit commit -m \"Message here\"\ngit push"
  }
]