[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Introduction to Statistical Learning using Python",
    "section": "Welcome",
    "text": "Welcome\nThis is a companion for the book Introduction to Statistical Learning with Python.\nThis website is being developed by the R4DS Online Learning Community. Follow along and join the community to participate.\nThis companion follows the R4DS Online Learning Community Code of Conduct."
  },
  {
    "objectID": "index.html#book-club-meetings",
    "href": "index.html#book-club-meetings",
    "title": "Introduction to Statistical Learning using Python",
    "section": "Book club meetings",
    "text": "Book club meetings\n\nEach week, a volunteer will present a chapter from the book.\n\nThis is the best way to learn the material.\n\nPresentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter.\nMore information about how to present is available in the GitHub repo.\nPresentations will be recorded and will be available on the R4DS Online Learning Community YouTube Channel."
  },
  {
    "objectID": "01_main.html#learning-objectives",
    "href": "01_main.html#learning-objectives",
    "title": "1. Introduction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nRecognize various types of statistical learning.\nUnderstand why this book is useful for you.\nBe able to read mathematical notation used throughout this book.\nDescribe the overall layout of this book.\nBe able to find data used in examples throughout the book."
  },
  {
    "objectID": "01_notes.html#what-is-statistical-learning",
    "href": "01_notes.html#what-is-statistical-learning",
    "title": "Notes",
    "section": "What is statistical learning?",
    "text": "What is statistical learning?\nStatistical learning is the theoretical foundation for machine learning framework. It makes connections between the fields of statistics, linear algebra and functional analysis.\nIn particular, the Statistical learning theory deals with the problem of finding a predictive function based on data and this is what is best known as supervised learning, in this book we will see more than just theory, as we will deal with unsupervised learning as well as making practical applications.\n\nSupervised: “Building a model to predict an output from inputs.”\n\nPredict wage from age, education, and year.\nPredict market direction from previous days' performance.\n\nUnsupervised: Inputs but no specific outputs, find relationships and structure.\n\nIdentify clusters within cancer cell lines."
  },
  {
    "objectID": "01_notes.html#why-islp",
    "href": "01_notes.html#why-islp",
    "title": "Notes",
    "section": "Why ISLP?",
    "text": "Why ISLP?\n\n“Facilitate the transition of statistical learning from an academic to a mainstream field.”\nMachine learning* is useful to everyone, let’s all learn enough to use it responsibly.\nPython “labs” make this make sense for this community!"
  },
  {
    "objectID": "01_notes.html#premises-of-islp",
    "href": "01_notes.html#premises-of-islp",
    "title": "Notes",
    "section": "Premises of ISLP",
    "text": "Premises of ISLP\nFrom Page 9 of the Introduction:\n\n“Many statistical learning methods are relevant and useful in a wide range of academic and non-academic disciplines, beyond just the statistical sciences.”\n“Statistical learning should not be viewed as a series of black boxes.”\n“While it is important to know what job is performed by each cog, it is not necessary to have the skills to construct the machine inside the box!”\n“We presume that the reader is interested in applying statistical learning methods to real-world problems.”"
  },
  {
    "objectID": "01_notes.html#notation",
    "href": "01_notes.html#notation",
    "title": "Notes",
    "section": "Notation",
    "text": "Notation\n\nn = number of observations (rows)\np = number of features/variables (columns)\nWe’ll come back here if we need to as we go!\nSome symbols they assume we know:\n\n\\(\\in\\) = “is an element of”, “in”\n\\({\\rm I\\!R}\\) = “real numbers”"
  },
  {
    "objectID": "01_notes.html#what-have-we-gotten-ourselves-into",
    "href": "01_notes.html#what-have-we-gotten-ourselves-into",
    "title": "Notes",
    "section": "What have we gotten ourselves into?",
    "text": "What have we gotten ourselves into?\nAn Introduction to Statistical Learning (ISL by James, Witten, Hastie and Tibshiraniis), is a collection of modern statistical methods for modeling and making predictions from real-world data.\nIt is a middle way between theoretical statistics and the practice of applying statistics to real-world problems.\nIt can be considered as a user manual, with self-contained Python labs, which lead you through the use of different methods for applying statistical analysis to different kinds of data.\n\n2: Terminology & main concepts\n3-4: Classic linear methods\n5: Resampling (so we can choose the best method)\n6: Modern updates to linear methods\n7+: Beyond Linearity (we can worry about details as we get there)"
  },
  {
    "objectID": "01_notes.html#wheres-the-data",
    "href": "01_notes.html#wheres-the-data",
    "title": "Notes",
    "section": "Where’s the data?",
    "text": "Where’s the data?\npip install ISLP\nWe’ll look at this data in more detail below."
  },
  {
    "objectID": "01_notes.html#some-useful-resources",
    "href": "01_notes.html#some-useful-resources",
    "title": "Notes",
    "section": "Some useful resources:",
    "text": "Some useful resources:\n\nthe book page: statlearning.com\npdf of the book: ISLRv2_website\nISLP labs\ncourse on edX: statistical-learning\nyoutube channel: playlists\nexercise solutions: applied solutions\n\nSome more theoretical resources:\n\nThe Elements of Statistical Learning (ESL, by Hastie, Tibshirani, and Friedman) ESLII"
  },
  {
    "objectID": "01_notes.html#what-is-covered-in-the-book",
    "href": "01_notes.html#what-is-covered-in-the-book",
    "title": "Notes",
    "section": "What is covered in the book?",
    "text": "What is covered in the book?\nThe book provides a series of toolkits classified as supervised or unsupervised techniques for understanding data.\nThe second edition of the book (2021) contains additions within the most updated statistical analysis.\n\n\n\n\n\nEditions"
  },
  {
    "objectID": "01_notes.html#how-is-the-book-divided",
    "href": "01_notes.html#how-is-the-book-divided",
    "title": "Notes",
    "section": "How is the book divided?",
    "text": "How is the book divided?\nThe book is divided into 13 chapters covering:\n\nIntroduction and Statistical Learning:\n\nSupervised Versus Unsupervised Learning\nRegression Versus Classification Problems\n\n\nLinear statistical learning\n\nLinear Regression:\n\nbasic concepts\nintroduction of K-nearest neighbor classifier\n\nClassification:\n\nlogistic regression\nlinear discriminant analysis\n\nResampling Methods:\n\ncross-validation\nthe bootstrap\n\nLinear Model Selection and Regularization: potential improvements over standard linear regression\n\nstepwise selection\nridge regression\nprincipal components regression\nthe lasso\n\n\nNon-linear statistical learning\n\nMoving Beyond Linearity:\n\nPolynomial Regression\nRegression Spline\nSmoothing Splines\nLocal Regression\nGeneralized Additive Models\n\nTree-Based Methods:\n\nDecision Trees\nBagging, Random Forests, Boosting, and Bayesian Additive Regression Trees\n\nSupport Vector Machines (linear and non-linear classification)\nDeep Learning (non-linear regression and classification)\nSurvival Analysis and Censored Data\nUnsupervised Learning:\n\nPrincipal components analysis\nK-means clustering\nHierarchical clustering\n\nMultiple Testing\n\nEach chapter includes 1 self-contained R lab on the topic"
  },
  {
    "objectID": "01_notes.html#some-examples-of-the-problems-addressed-with-statistical-analysis",
    "href": "01_notes.html#some-examples-of-the-problems-addressed-with-statistical-analysis",
    "title": "Notes",
    "section": "Some examples of the problems addressed with statistical analysis",
    "text": "Some examples of the problems addressed with statistical analysis\n\nIdentify the risk factors for some type of cancers\nPredict whether someone will have a hearth attack on the basis of demographic, diet, and clinical measurements\nEmail spam detection\nClassify a tissue sample into one of several cancer classes, based on a gene expression profile\nEstablish the relationship between salary and demographic variables in population survey data\n\n(source)"
  },
  {
    "objectID": "01_notes.html#datasets-provided-in-the-islp-package",
    "href": "01_notes.html#datasets-provided-in-the-islp-package",
    "title": "Notes",
    "section": "Datasets provided in the ISLP package",
    "text": "Datasets provided in the ISLP package\nThe book provides the ISLP Python package with all the datasets needed the analysis.\n\n\n\n\n\nDatasets in ISLP package"
  },
  {
    "objectID": "01_video.html#cohort-01",
    "href": "01_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "02_main.html#learning-objectives",
    "href": "02_main.html#learning-objectives",
    "title": "2. Statistical Learning",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nCompare supervised vs unsupervised learning.\nCompare regression vs classification problems.\nMeasure the accuracy of regression model fits.\nMeasure the accuracy of classification model fits.\nDescribe the trade-off between more accurate models and more interpretable models.\nCompare Parametric vs Nonparametric models\nUnderstand overfitting.\nDescribe how bias and variance contribute to the model error."
  },
  {
    "objectID": "02_notes.html#what-is-statistical-learning",
    "href": "02_notes.html#what-is-statistical-learning",
    "title": "Notes",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nIn this chapter will deal with developing an accurate model that can be used to predict some value.\nNotation:\n\nInput variables: \\(X_1, \\cdots, X_p\\)\nAlso known as predictors, features, independent variables.\nOutput variable: \\(Y\\)\nAlso known as response or dependent variable.\n\nWe assume there is some relationship between \\(Y\\) and \\(X = \\left( X_1, \\cdots, X_p \\right)\\), which we write as:\n\\[Y = f(X) + \\epsilon\\]\n, where \\(\\epsilon\\) is a random error term which is independent from \\(X\\) and has mean zero; and, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y\\) .\n\n\n\n\n\nIncome data set\n\n\n\n\nIn essence, statistical learning deals with different approaches to estimate \\(f\\) .\n\nWhy estimate \\(f\\)?\nTwo main reasons to estimate \\(f\\):\n\nPrediction\n\nPredict \\(Y\\) using a set of inputs \\(X\\) .\nRepresentation: \\(\\hat{Y}= \\hat{f}(X)\\), where \\(\\hat{f}\\) represents our estimate for \\(f\\), and \\(\\hat{Y}\\) our prediction for \\(Y\\) .*\nIn this setting, \\(\\hat{f}\\) is often treated as a black-box, meaning we don’t mind not knowing the exact form of \\(\\hat{f}\\), if it generates accurate predictions for \\(Y\\) .\n\\(\\hat{Y}\\)’s accuracy depends on:\n\nReducible error\n\nDue to \\(\\hat{f}\\) not being a perfect estimate for \\(f\\).\nCan be reduced by using a proper statistical learning technique.\n\nIrreducible error\n\nDue to \\(\\epsilon\\) and its variability.\n\\(\\epsilon\\) is independent from \\(X\\), so no matter how well we estimate \\(f\\), we can’t reduce this error.\n\n\nThe quantity \\(\\epsilon\\) may contain unmeasured variables useful for predicting \\(Y\\); or, may contain unmeasure variation, so no prediction model will be perfect.\nMathematical form, after choosing predictors \\(X\\) and an estimate \\(\\hat{f}\\):\n\n\\[\nE( Y - \\hat{Y} )^2 =\nE(f(X) + \\epsilon - \\hat{f}(X))^2 =\n\\underbrace{[f(X) - \\hat{f}(X)]^2}_{reducible} +\n\\underbrace{\\text{ Var}(\\epsilon)}_{irreducible}\\; .\n\\]\nIn practice, we almost always don’t know how \\(\\epsilon\\)’s variability affects our model, so, in this boook, we will focus on techniques for estimating \\(f\\) .\n\n\nInference\nIn this case, we are interested in understanding the association between \\(Y\\) and \\(X_1, \\cdots, X_p\\).\n\nFor example:\n\nWhich predictors are most associated with response?\nWhat is the relationship between the response and each predictor?\nCan such relationship be summarized via a linear equation, or is it more complex?\n\n\nThe exact form of \\(\\hat{f}\\) is required.\nLinear models allow for easier interpretability, but can lack in prediction accuracy; while, non-linear models can be more accurate, but less interpretable.\n\n\n\nHow do we estimate \\(f\\) ?\n\nFirst, let’s agree on some conventions:\n\n\\(n\\) : Number of observations.\n\\(x_{ij}\\): Value of the \\(j\\text{th}\\) predictor, for \\(i\\text{th}\\) observation.\n\\(y_i\\) : Response variable for \\(i\\text{th}\\) observation.\nTraining data:\n\nSet of observations.\nUsed to esmitate \\(f\\).\n\\(\\left\\{ (x_1, y_1), \\cdots, (x_n, y_n) \\right\\}\\), where \\(x_i = (x_{i1}, \\cdots, x_{ip})^T\\) .\n\n\nGoal: Find a function \\(\\hat{f}\\) such that \\(Y\\approx\\hat{f}(X)\\) for any observation \\((X,Y)\\) .\nMost statistical methods for achieving this goal can be characterized as either parametric or non-parametric.\n\n\nParametric methods\n\nSteps:\n\nMake an assumption about the form of \\(f\\).\nIt could be linear (\\(f(X) = \\beta_0 + \\beta_1 X_1 + \\cdot + \\beta_p X_p,\\) parameters \\(\\beta_0, \\cdots, \\beta_p\\) to be estimated) or not.\nThe model has been selected.\nNow, we need a procedure to fit the model using the training data.\nThe most common of such fitting procedures is called (ordinary) least squares.\n\nVia these steps, the problem of estimating \\(f\\) has been reduced to a problem of estimating a set of parameters.\nWe can make the models more flexible via considering a greater number of parameters, but, this can lead to overfitting the data, that is, following the errors/noise too closely, which will not yield accurate estimates of the response for observations outside of the original training data.\n\n\n\n\nNon-parametric methods\n\nNo assumptions about the form of \\(f\\) are made.\nInstead, we seek an estimate of \\(f\\) which that gets as close to the data point as possible.\nHas the potential to fit a wider range of possible forms for \\(f\\).\nTipically requires a very large number of observations (compared to paramatric approach) in order to accurately estimate \\(f\\).\n\n\n\n\nThe trade-off between prediction accuracy and model interpretability\nWe’ve seen that parametric models are usually restrictive; and, non-parametric models, flexible. However:\n\nRestrictive models are usually more interpretable, so they are useful for inference.\nFlexible models can be difficult to interpret, due to the complexity of \\(\\hat{f}\\).\n\nDespite this, we will often obtain more accurate predictions usinf a less flexible method, due to the potential for overfitting the data in highly flexible models.\n\n\nSupervised vs Unsupervised Learning\nIn supervised learning, we wish to fit a model that relates inputs/predictors to some output.\nIn unsupervised learning, we lack a reponse/variable to predict. Instead, we seek to understand the relationships between the variables or between the observations.\nThere are instances where a mix of such methods is required (semi-supervised learning problems), but such topic will not be covered in this book.\n\n\nRegression vs Classification problems\n\nIf the response is …\n\nQuantitative, then, it’s a regression problem.\nCategorical, then, it’s a classification problem.\n\nMost of the methods covered in this book can be applied regardless of the predictor variable type, but the categorical variables will require some pre-processing."
  },
  {
    "objectID": "02_notes.html#assessing-model-accuracy",
    "href": "02_notes.html#assessing-model-accuracy",
    "title": "Notes",
    "section": "Assessing model accuracy",
    "text": "Assessing model accuracy\n\nThere is no best method for Statistical Learning, the method’s efficacy can depend on the data set.\nFor a specific data set, how do we select the best Statistics approach?\n\n\nMeasuring the quality of fit\n\nThe performance of a statistical learning method can be evaluated comparing the predictions of the model, with their true/real response.\nMost commonly used measure for this:\n\nMean squared error\n\\(\\text{ MSE } = \\dfrac{1}{n}\\displaystyle{ \\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2 }\\)\nSmall MSE means that the predicted and the true responses are very close.\n\nWe want the model to accurately predict unseen data (testing data), not so much the training data, where the response is already known.\nThe best model will be the one which produces the lowest test MSE, not the lowest training MSE.\nIt’s not true that the model with lowest training MSE will also have the lowest test MSE.\n\n\n\n\n\n\nTraining MSE vs Test MSE\n\n\n\n\n\nFundamental property: For any data set and any statistical learning method used, as the flexibility of the statistical learning method increases:\n\nThe training MSE decreases monotonically.\nThe test MSE graph has a U-shape.\n\n\n\nAs model flexibility increases, training MSE will decrease, but the test MSE may not.\n\n\nSmall training MSE but big test MSE implies having overfitted the data.\nRegardless of overfitting or not, we almost always expect \\(\\text{training MSE } &lt; \\text{ testing MSE }\\), beacuse most statistical learning methods seek to minimize the training MSE.\nEstimating test MSE is very difficult, usually because lack of data. Later in this book, we’ll discuss approaches to estimate the mininum point for the test MSE curve.\n\n\n\nThe Bias-Variance Trade-off\n\nDefinition: The expected test MSE at \\(x_0\\) (\\(E(y_0 - \\hat{f}(x_0))^2\\)) refers to the averga test MSE that we would obtain after repeatedly estimating \\(f\\) using a large number of training sets, and tested each esimate at \\(x_0\\).\nDefinition: The variance of a statistical learning method which produces an estimate \\(\\hat{f}\\) refers to how the estimate function changes, for different training sets.\nDefinition: Bias refers to the error generated by approximating a possibly complicated model (like in real-life usually), by a much simpler one … (how \\(f\\) and the possibles \\(\\hat{f}\\) differ).\nAs a general rule, the more flexible a statistical method, the higher its variance and lower its bias.\nFor any given value \\(x_0\\), the following can be proved:\n\n\\[\nE(y_0 - \\hat{f}(x_0))^2 = \\text{Var}(\\hat{f}(x_0)) + \\text{Bias}(\\hat{f}(x_0))^2 + \\text{ Var }(\\epsilon)\n\\]\n\nDue to variance and squared bias being non negative, the previous equation implies that, to minimize the expected test error, we require a statistical learnig method which achieves low variance and low bias.\nThe tradeoff:\n\nExtremely low bias but high variance: For example, draw a line which passes over every single point in the training data.\nExtremely low variance but high bias: For example, fit a horizontal line to the data.\n\nThe challenge lies in finding a method for which both the variance and the squared bias are low.\n\n\nIn a real-life situation, \\(f\\) is usually unkwon, so it’s not possible to explicitly compute the test MSE, bias or variance of a statistical method.\nThe test MSE can be estimated using cross-validation, but we’ll discuss it later in this book.\n\n\n\nThe Classification setting\nLet’s see how the concepts recently discussed change when we the prediction is a categorical variable.\nThe most common approach for quantifying the accuracy of our estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes made by applying \\(\\hat{f}\\) to the training observations:\n\\[\n\\dfrac{1}{n}\\displaystyle{ \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i)}\n\\]\n, where \\(I\\) is \\(1\\) when \\(y_i = \\hat{y}_i\\), and \\(0\\) otherwise.\n\nThe test error rate is defined as \\(\\text{ Average}(I(y_i \\neq \\hat{y}_i))\\), where the average is computed by comparing the predictions \\(\\hat{y}_i\\) with the true response \\(y_i\\).\nA good classifier is one for which the test error is smallest."
  },
  {
    "objectID": "02_video.html#cohort-01",
    "href": "02_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "02_exercises.html#this-exercise-relates-to-the-college-data-set-which-can-be-found-in-the-file-college.csv-on-the-book-website.-it-contains-a-number-of-variables-for-777-different-universities-and-colleges-in-the-us.-the-variables-are",
    "href": "02_exercises.html#this-exercise-relates-to-the-college-data-set-which-can-be-found-in-the-file-college.csv-on-the-book-website.-it-contains-a-number-of-variables-for-777-different-universities-and-colleges-in-the-us.-the-variables-are",
    "title": "Applied Exercises",
    "section": "8. This exercise relates to the College data set, which can be found in the file College.csv on the book website. It contains a number of variables for 777 different universities and colleges in the US. The variables are",
    "text": "8. This exercise relates to the College data set, which can be found in the file College.csv on the book website. It contains a number of variables for 777 different universities and colleges in the US. The variables are\n\nPrivate : Public/private indicator\nApps : Number of applications received\nAccept : Number of applicants accepted\nEnroll : Number of new students enrolled\nTop10perc : New students from top 10 % of high school class\nTop25perc : New students from top 25 % of high school class\nF.Undergrad : Number of full-time undergraduates\nP.Undergrad : Number of part-time undergraduates\nOutstate : Out-of-state tuition\nRoom.Board : Room and board costs\nBooks : Estimated book costs\nPersonal : Estimated personal spending\nPhD : Percent of faculty with Ph.D.s\nTerminal : Percent of faculty with terminal degree\nS.F.Ratio : Student/faculty ratio\nperc.alumni : Percent of alumni who donate\nExpend : Instructional expenditure per student\nGrad.Rate : Graduation rate\n\nBefore reading the data into Python, it can be viewed in Excel or a text editor.\n\n(a) Use the pd.read_csv() function to read the data into Python. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.\n\ncollege = pd.read_csv('ISLP_data/College.csv')\ncollege\n\n                         Unnamed: 0 Private   Apps  Accept  Enroll  Top10perc  \\\n0      Abilene Christian University     Yes   1660    1232     721         23   \n1                Adelphi University     Yes   2186    1924     512         16   \n2                    Adrian College     Yes   1428    1097     336         22   \n3               Agnes Scott College     Yes    417     349     137         60   \n4         Alaska Pacific University     Yes    193     146      55         16   \n..                              ...     ...    ...     ...     ...        ...   \n772         Worcester State College      No   2197    1515     543          4   \n773               Xavier University     Yes   1959    1805     695         24   \n774  Xavier University of Louisiana     Yes   2097    1915     695         34   \n775                 Yale University     Yes  10705    2453    1317         95   \n776    York College of Pennsylvania     Yes   2989    1855     691         28   \n\n     Top25perc  F.Undergrad  P.Undergrad  Outstate  Room.Board  Books  \\\n0           52         2885          537      7440        3300    450   \n1           29         2683         1227     12280        6450    750   \n2           50         1036           99     11250        3750    400   \n3           89          510           63     12960        5450    450   \n4           44          249          869      7560        4120    800   \n..         ...          ...          ...       ...         ...    ...   \n772         26         3089         2029      6797        3900    500   \n773         47         2849         1107     11520        4960    600   \n774         61         2793          166      6900        4200    617   \n775         99         5217           83     19840        6510    630   \n776         63         2988         1726      4990        3560    500   \n\n     Personal  PhD  Terminal  S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n0        2200   70        78       18.1           12    7041         60  \n1        1500   29        30       12.2           16   10527         56  \n2        1165   53        66       12.9           30    8735         54  \n3         875   92        97        7.7           37   19016         59  \n4        1500   76        72       11.9            2   10922         15  \n..        ...  ...       ...        ...          ...     ...        ...  \n772      1200   60        60       21.0           14    4469         40  \n773      1250   73        75       13.3           31    9189         83  \n774       781   67        75       14.4           20    8323         49  \n775      2115   96        96        5.8           49   40386         99  \n776      1250   75        75       18.1           28    4509         99  \n\n[777 rows x 19 columns]\n\n\n\n\n(b) Look at the data used in the notebook by creating and running a new cell with just the code college in it. You should notice that the first column is just the name of each university in a column named something like Unnamed: 0. We don’t really want pandas to treat this as data. However, it may be handy to have these names for later. Try the following commands and similarly look at the resulting data frames:\n\n\ncollege2 = pd.read_csv('ISLP_data/College.csv', index_col=0)\n#college2\n\ncollege3 = college.rename({'Unnamed: 0': 'college'},\n  axis=1)\n#college3\n\ncollege3 = college3.set_index('college')\n#college3\n\nThis has used the first column in the file as an index for the data frame. This means that pandas has given each row a name corresponding to the appropriate university. Now you should see that the first data column is Private. Note that the names of the colleges appear on the left of the table. We also introduced a new python object above: a dictionary, which is specified by (key, value) pairs. Keep your modified version of the data with the following:\n\ncollege = college3\ncollege\n\n                               Private   Apps  Accept  Enroll  Top10perc  \\\ncollege                                                                    \nAbilene Christian University       Yes   1660    1232     721         23   \nAdelphi University                 Yes   2186    1924     512         16   \nAdrian College                     Yes   1428    1097     336         22   \nAgnes Scott College                Yes    417     349     137         60   \nAlaska Pacific University          Yes    193     146      55         16   \n...                                ...    ...     ...     ...        ...   \nWorcester State College             No   2197    1515     543          4   \nXavier University                  Yes   1959    1805     695         24   \nXavier University of Louisiana     Yes   2097    1915     695         34   \nYale University                    Yes  10705    2453    1317         95   \nYork College of Pennsylvania       Yes   2989    1855     691         28   \n\n                                Top25perc  F.Undergrad  P.Undergrad  Outstate  \\\ncollege                                                                         \nAbilene Christian University           52         2885          537      7440   \nAdelphi University                     29         2683         1227     12280   \nAdrian College                         50         1036           99     11250   \nAgnes Scott College                    89          510           63     12960   \nAlaska Pacific University              44          249          869      7560   \n...                                   ...          ...          ...       ...   \nWorcester State College                26         3089         2029      6797   \nXavier University                      47         2849         1107     11520   \nXavier University of Louisiana         61         2793          166      6900   \nYale University                        99         5217           83     19840   \nYork College of Pennsylvania           63         2988         1726      4990   \n\n                                Room.Board  Books  Personal  PhD  Terminal  \\\ncollege                                                                      \nAbilene Christian University          3300    450      2200   70        78   \nAdelphi University                    6450    750      1500   29        30   \nAdrian College                        3750    400      1165   53        66   \nAgnes Scott College                   5450    450       875   92        97   \nAlaska Pacific University             4120    800      1500   76        72   \n...                                    ...    ...       ...  ...       ...   \nWorcester State College               3900    500      1200   60        60   \nXavier University                     4960    600      1250   73        75   \nXavier University of Louisiana        4200    617       781   67        75   \nYale University                       6510    630      2115   96        96   \nYork College of Pennsylvania          3560    500      1250   75        75   \n\n                                S.F.Ratio  perc.alumni  Expend  Grad.Rate  \ncollege                                                                    \nAbilene Christian University         18.1           12    7041         60  \nAdelphi University                   12.2           16   10527         56  \nAdrian College                       12.9           30    8735         54  \nAgnes Scott College                   7.7           37   19016         59  \nAlaska Pacific University            11.9            2   10922         15  \n...                                   ...          ...     ...        ...  \nWorcester State College              21.0           14    4469         40  \nXavier University                    13.3           31    9189         83  \nXavier University of Louisiana       14.4           20    8323         49  \nYale University                       5.8           49   40386         99  \nYork College of Pennsylvania         18.1           28    4509         99  \n\n[777 rows x 18 columns]\n\n\n\n\n(c) Use the describe() method to produce a numerical summary of the variables in the data set.\n\ncollege.describe()\n\n               Apps        Accept       Enroll   Top10perc   Top25perc  \\\ncount    777.000000    777.000000   777.000000  777.000000  777.000000   \nmean    3001.638353   2018.804376   779.972973   27.558559   55.796654   \nstd     3870.201484   2451.113971   929.176190   17.640364   19.804778   \nmin       81.000000     72.000000    35.000000    1.000000    9.000000   \n25%      776.000000    604.000000   242.000000   15.000000   41.000000   \n50%     1558.000000   1110.000000   434.000000   23.000000   54.000000   \n75%     3624.000000   2424.000000   902.000000   35.000000   69.000000   \nmax    48094.000000  26330.000000  6392.000000   96.000000  100.000000   \n\n        F.Undergrad   P.Undergrad      Outstate   Room.Board        Books  \\\ncount    777.000000    777.000000    777.000000   777.000000   777.000000   \nmean    3699.907336    855.298584  10440.669241  4357.526384   549.380952   \nstd     4850.420531   1522.431887   4023.016484  1096.696416   165.105360   \nmin      139.000000      1.000000   2340.000000  1780.000000    96.000000   \n25%      992.000000     95.000000   7320.000000  3597.000000   470.000000   \n50%     1707.000000    353.000000   9990.000000  4200.000000   500.000000   \n75%     4005.000000    967.000000  12925.000000  5050.000000   600.000000   \nmax    31643.000000  21836.000000  21700.000000  8124.000000  2340.000000   \n\n          Personal         PhD    Terminal   S.F.Ratio  perc.alumni  \\\ncount   777.000000  777.000000  777.000000  777.000000   777.000000   \nmean   1340.642214   72.660232   79.702703   14.089704    22.743887   \nstd     677.071454   16.328155   14.722359    3.958349    12.391801   \nmin     250.000000    8.000000   24.000000    2.500000     0.000000   \n25%     850.000000   62.000000   71.000000   11.500000    13.000000   \n50%    1200.000000   75.000000   82.000000   13.600000    21.000000   \n75%    1700.000000   85.000000   92.000000   16.500000    31.000000   \nmax    6800.000000  103.000000  100.000000   39.800000    64.000000   \n\n             Expend  Grad.Rate  \ncount    777.000000  777.00000  \nmean    9660.171171   65.46332  \nstd     5221.768440   17.17771  \nmin     3186.000000   10.00000  \n25%     6751.000000   53.00000  \n50%     8377.000000   65.00000  \n75%    10830.000000   78.00000  \nmax    56233.000000  118.00000  \n\n\n\n\n(d) Use the pd.plotting.scatter_matrix() function to produce a scatterplot matrix of the first columns [Top10perc, Apps, Enroll]. Recall that you can reference a list C of columns of a data frame A using A[C].\n\n#fig, ax = subplots(figsize=(8, 8))\npd.plotting.scatter_matrix(college[['Top10perc','Apps','Enroll']])\n\narray([[&lt;AxesSubplot:xlabel='Top10perc', ylabel='Top10perc'&gt;,\n        &lt;AxesSubplot:xlabel='Apps', ylabel='Top10perc'&gt;,\n        &lt;AxesSubplot:xlabel='Enroll', ylabel='Top10perc'&gt;],\n       [&lt;AxesSubplot:xlabel='Top10perc', ylabel='Apps'&gt;,\n        &lt;AxesSubplot:xlabel='Apps', ylabel='Apps'&gt;,\n        &lt;AxesSubplot:xlabel='Enroll', ylabel='Apps'&gt;],\n       [&lt;AxesSubplot:xlabel='Top10perc', ylabel='Enroll'&gt;,\n        &lt;AxesSubplot:xlabel='Apps', ylabel='Enroll'&gt;,\n        &lt;AxesSubplot:xlabel='Enroll', ylabel='Enroll'&gt;]], dtype=object)\n\n#plt.show()\n\n\n\n\n\n\n(e) Use the boxplot() method of college to produce side-by-side boxplots of Outstate versus Private.\n\n\n(f) Create a new qualitative variable, called Elite, by binning the Top10perc variable into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.\n\ncollege['Elite'] = pd.cut(college['Top10perc'],\n  [0,0.5,1],\n  labels=['No', 'Yes'])\n\nUse the value_counts() method of college['Elite'] to see how many elite universities there are. Finally, use the boxplot() method again to produce side-by-side boxplots of Outstate versus Elite.\n\ncollege['Elite'].value_counts()\n\nYes    3\nNo     0\nName: Elite, dtype: int64\n\n\n\n\n(g) Use the plot.hist() method of college to produce some histograms with difering numbers of bins for a few of the quantitative variables. The command plt.subplots(2, 2) may be useful: it will divide the plot window into four regions so that four plots can be made simultaneously. By changing the arguments you can divide the screen up in other combinations.\n\n\n(h) Continue exploring the data, and provide a brief summary of what you discover."
  },
  {
    "objectID": "02_exercises.html#this-exercise-involves-the-auto-data-set-studied-in-the-lab.-make-sure-that-the-missing-values-have-been-removed-from-the-data.",
    "href": "02_exercises.html#this-exercise-involves-the-auto-data-set-studied-in-the-lab.-make-sure-that-the-missing-values-have-been-removed-from-the-data.",
    "title": "Applied Exercises",
    "section": "9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.",
    "text": "9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.\n\nAuto = pd.read_csv('ISLP_data/Auto.csv',\n                    na_values=['?'])\nAuto\n\n      mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n0    18.0          8         307.0       130.0    3504          12.0    70   \n1    15.0          8         350.0       165.0    3693          11.5    70   \n2    18.0          8         318.0       150.0    3436          11.0    70   \n3    16.0          8         304.0       150.0    3433          12.0    70   \n4    17.0          8         302.0       140.0    3449          10.5    70   \n..    ...        ...           ...         ...     ...           ...   ...   \n392  27.0          4         140.0        86.0    2790          15.6    82   \n393  44.0          4          97.0        52.0    2130          24.6    82   \n394  32.0          4         135.0        84.0    2295          11.6    82   \n395  28.0          4         120.0        79.0    2625          18.6    82   \n396  31.0          4         119.0        82.0    2720          19.4    82   \n\n     origin                       name  \n0         1  chevrolet chevelle malibu  \n1         1          buick skylark 320  \n2         1         plymouth satellite  \n3         1              amc rebel sst  \n4         1                ford torino  \n..      ...                        ...  \n392       1            ford mustang gl  \n393       2                  vw pickup  \n394       1              dodge rampage  \n395       1                ford ranger  \n396       1                 chevy s-10  \n\n[397 rows x 9 columns]\n\n\n\n(a) Which of the predictors are quantitative, and which are qualitative?\nMpg, Displacement, Horsepower, Weight and Acceleration are quantitative. Cylinders, Year, Origin, and Name are qualitative.\n\nAuto.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 397 entries, 0 to 396\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           397 non-null    float64\n 1   cylinders     397 non-null    int64  \n 2   displacement  397 non-null    float64\n 3   horsepower    392 non-null    float64\n 4   weight        397 non-null    int64  \n 5   acceleration  397 non-null    float64\n 6   year          397 non-null    int64  \n 7   origin        397 non-null    int64  \n 8   name          397 non-null    object \ndtypes: float64(4), int64(4), object(1)\nmemory usage: 28.0+ KB\n\n\n\nAuto['cylinders'] = Auto['cylinders'].astype('object') \nAuto['cylinders']\n\n0      8\n1      8\n2      8\n3      8\n4      8\n      ..\n392    4\n393    4\n394    4\n395    4\n396    4\nName: cylinders, Length: 397, dtype: object\n\n\n\n\n(b) What is the range of each quantitative predictor? You can answer this using the min() and max() methods in numpy.\n\nmpg_min = Auto['mpg'].min( )\nmpg_max = Auto['mpg'].max( )\n\nprint('The min and max miles per gallon are', (mpg_min, mpg_max))\n\nThe min and max miles per gallon are (9.0, 46.6)\n\n\n\ndsp_min = Auto['displacement'].min( )\ndsp_max = Auto['displacement'].max( )\n\nprint('The min and max displacement are', (dsp_min, dsp_max))\n\nThe min and max displacement are (68.0, 455.0)\n\n\n\nhpwr_min = Auto['horsepower'].min( )\nhpwr_max = Auto['horsepower'].max( )\n\nprint('The min and max horsepower are', (hpwr_min, hpwr_max))\n\nThe min and max horsepower are (46.0, 230.0)\n\n\n\nwt_min = Auto['weight'].min( )\nwt_max = Auto['weight'].max( )\n\nprint('The min and max weights are', (wt_min, wt_max))\n\nThe min and max weights are (1613, 5140)\n\n\n\nacc_min = Auto['acceleration'].min( )\nacc_max = Auto['acceleration'].max( )\n\nprint('The min and max accelerations are', (acc_min, acc_max))\n\nThe min and max accelerations are (8.0, 24.8)\n\n\n\n\n(c) What is the mean and standard deviation of each quantitative predictor?\n\nmpg_mean = Auto['mpg'].mean( )\nmpg_sd = Auto['mpg'].std( )\n\nprint('The mean and standard deviation of miles per gallon are', mpg_mean,'and', mpg_sd)\n\nThe mean and standard deviation of miles per gallon are 23.515869017632248 and 7.825803928946562\n\n\n\ndsp_mean = Auto['displacement'].mean( )\ndsp_sd = Auto['displacement'].std( )\n\nprint('The mean and standard deviation of weight are', dsp_mean,'and', dsp_sd)\n\nThe mean and standard deviation of weight are 193.53274559193954 and 104.37958329992945\n\n\n\nhpwr_mean = Auto['horsepower'].mean( )\nhpwr_sd = Auto['horsepower'].std( )\n\nprint('The mean and standard deviation of horsepower are', hpwr_mean,'and', hpwr_sd)\n\nThe mean and standard deviation of horsepower are 104.46938775510205 and 38.49115993282855\n\n\n\nwt_mean = Auto['weight'].mean( )\nwt_sd = Auto['weight'].std( )\n\nprint('The mean and standard deviation of weight are', wt_mean,'and', wt_sd)\n\nThe mean and standard deviation of weight are 2970.2619647355164 and 847.9041194897246\n\n\n\nacc_mean = Auto['acceleration'].mean( )\nacc_sd = Auto['acceleration'].std( )\n\nprint('The mean and standard deviation of acceleration are', acc_mean,'and', acc_sd)\n\nThe mean and standard deviation of acceleration are 15.555667506297214 and 2.7499952929761515\n\n\n\n\n(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?\n\nAuto_new = Auto.drop(Auto.index[10:85])\nAuto_new\n\n      mpg cylinders  displacement  horsepower  weight  acceleration  year  \\\n0    18.0         8         307.0       130.0    3504          12.0    70   \n1    15.0         8         350.0       165.0    3693          11.5    70   \n2    18.0         8         318.0       150.0    3436          11.0    70   \n3    16.0         8         304.0       150.0    3433          12.0    70   \n4    17.0         8         302.0       140.0    3449          10.5    70   \n..    ...       ...           ...         ...     ...           ...   ...   \n392  27.0         4         140.0        86.0    2790          15.6    82   \n393  44.0         4          97.0        52.0    2130          24.6    82   \n394  32.0         4         135.0        84.0    2295          11.6    82   \n395  28.0         4         120.0        79.0    2625          18.6    82   \n396  31.0         4         119.0        82.0    2720          19.4    82   \n\n     origin                       name  \n0         1  chevrolet chevelle malibu  \n1         1          buick skylark 320  \n2         1         plymouth satellite  \n3         1              amc rebel sst  \n4         1                ford torino  \n..      ...                        ...  \n392       1            ford mustang gl  \n393       2                  vw pickup  \n394       1              dodge rampage  \n395       1                ford ranger  \n396       1                 chevy s-10  \n\n[322 rows x 9 columns]\n\n\n\nmpg_min = Auto_new['mpg'].min( )\nmpg_max = Auto_new['mpg'].max( )\n\nprint('The min and max miles per gallon of the subsetted data are', (mpg_min, mpg_max))\n\nThe min and max miles per gallon of the subsetted data are (11.0, 46.6)\n\nmpg_mean = Auto_new['mpg'].mean( )\nmpg_sd = Auto_new['mpg'].std( )\n\nprint('The mean and standard deviation of miles per gallon of the subsetted data are', mpg_mean,'and', mpg_sd)\n\nThe mean and standard deviation of miles per gallon of the subsetted data are 24.40931677018633 and 7.913357147165568\n\n\n\ndsp_min = Auto_new['displacement'].min( )\ndsp_max = Auto_new['displacement'].max( )\n\nprint('The min and max displacement of the subsetted data are', (dsp_min, dsp_max))\n\nThe min and max displacement of the subsetted data are (68.0, 455.0)\n\ndsp_mean = Auto_new['displacement'].mean( )\ndsp_sd = Auto_new['displacement'].std( )\n\nprint('The mean and standard deviation of weight of the subsetted data are', dsp_mean,'and', dsp_sd)\n\nThe mean and standard deviation of weight of the subsetted data are 187.6801242236025 and 100.12092459330134\n\n\n\nhpwr_min = Auto['horsepower'].min( )\nhpwr_max = Auto['horsepower'].max( )\n\nprint('The min and max horsepower of the subsetted data are', (hpwr_min, hpwr_max))\n\nThe min and max horsepower of the subsetted data are (46.0, 230.0)\n\nhpwr_mean = Auto['horsepower'].mean( )\nhpwr_sd = Auto['horsepower'].std( )\n\nprint('The mean and standard deviation of horsepower of the subsetted data are', hpwr_mean,'and', hpwr_sd)\n\nThe mean and standard deviation of horsepower of the subsetted data are 104.46938775510205 and 38.49115993282855\n\n\n\nwt_min = Auto['weight'].min( )\nwt_max = Auto['weight'].max( )\n\nprint('The min and max weights of the subsetted data are', (wt_min, wt_max))\n\nThe min and max weights of the subsetted data are (1613, 5140)\n\nwt_mean = Auto['weight'].mean( )\nwt_sd = Auto['weight'].std( )\n\nprint('The mean and standard deviation of weight of the subsetted data are', wt_mean,'and', wt_sd)\n\nThe mean and standard deviation of weight of the subsetted data are 2970.2619647355164 and 847.9041194897246\n\n\n\nacc_min = Auto['acceleration'].min( )\nacc_max = Auto['acceleration'].max( )\n\nprint('The min and max accelerations of the subsetted data are', (acc_min, acc_max))\n\nThe min and max accelerations of the subsetted data are (8.0, 24.8)\n\nacc_mean = Auto['acceleration'].mean( )\nacc_sd = Auto['acceleration'].std( )\n\nprint('The mean and standard deviation of acceleration of the subsetted data are', acc_mean,'and', acc_sd)\n\nThe mean and standard deviation of acceleration of the subsetted data are 15.555667506297214 and 2.7499952929761515\n\n\n\n\n(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.\n\n\n(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer."
  },
  {
    "objectID": "02_exercises.html#this-exercise-involves-the-boston-housing-data-set.",
    "href": "02_exercises.html#this-exercise-involves-the-boston-housing-data-set.",
    "title": "Applied Exercises",
    "section": "10. This exercise involves the Boston housing data set.",
    "text": "10. This exercise involves the Boston housing data set.\n\n(a) To begin, load in the Boston data set, which is part of the ISLP library.\n\nBoston = load_data(\"Boston\")\nBoston\n\n        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n\n     ptratio  lstat  medv  \n0       15.3   4.98  24.0  \n1       17.8   9.14  21.6  \n2       17.8   4.03  34.7  \n3       18.7   2.94  33.4  \n4       18.7   5.33  36.2  \n..       ...    ...   ...  \n501     21.0   9.67  22.4  \n502     21.0   9.08  20.6  \n503     21.0   5.64  23.9  \n504     21.0   6.48  22.0  \n505     21.0   7.88  11.9  \n\n[506 rows x 13 columns]\n\n\n\n\n(b) How many rows are in this data set? How many columns? What do the rows and columns represent?\n\n\n(c) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your fndings.\n\n\n(d) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\n\n(e) Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.\n\n\n(f) How many of the suburbs in this data set bound the Charles river?\n\n\n(g) What is the median pupil-teacher ratio among the towns in this data set?\n\n\n(h) Which suburb of Boston has lowest median value of owneroccupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your fndings.\n\n\n(i) In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling."
  },
  {
    "objectID": "03_main.html#learning-objectives",
    "href": "03_main.html#learning-objectives",
    "title": "3. Linear Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "03_video.html#cohort-01",
    "href": "03_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "04_main.html#learning-objectives",
    "href": "04_main.html#learning-objectives",
    "title": "4. Classification",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "04_video.html#cohort-01",
    "href": "04_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "05_main.html#learning-objectives",
    "href": "05_main.html#learning-objectives",
    "title": "5. Resampling Methods",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "05_video.html#cohort-01",
    "href": "05_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "06_main.html#learning-objectives",
    "href": "06_main.html#learning-objectives",
    "title": "6. Linear Model Selection and Regularization",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "06_video.html#cohort-01",
    "href": "06_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "07_main.html#learning-objectives",
    "href": "07_main.html#learning-objectives",
    "title": "7. Moving Beyond Linearity",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "07_video.html#cohort-01",
    "href": "07_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "08_main.html#learning-objectives",
    "href": "08_main.html#learning-objectives",
    "title": "8. Tree-Based Methods",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "08_video.html#cohort-01",
    "href": "08_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "09_main.html#learning-objectives",
    "href": "09_main.html#learning-objectives",
    "title": "9. Support Vector Machines",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "09_video.html#cohort-01",
    "href": "09_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "10_main.html#learning-objectives",
    "href": "10_main.html#learning-objectives",
    "title": "10. Deep Learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "10_video.html#cohort-01",
    "href": "10_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "11_main.html#learning-objectives",
    "href": "11_main.html#learning-objectives",
    "title": "11. Survival Analysis and Censored Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "11_video.html#cohort-01",
    "href": "11_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "12_main.html#learning-objectives",
    "href": "12_main.html#learning-objectives",
    "title": "12. Unsupervised Learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "12_video.html#cohort-01",
    "href": "12_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "13_main.html#learning-objectives",
    "href": "13_main.html#learning-objectives",
    "title": "13. Multiple Testing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nitem 1\nitem 2\nitem 3"
  },
  {
    "objectID": "13_video.html#cohort-01",
    "href": "13_video.html#cohort-01",
    "title": "Video",
    "section": "Cohort 01",
    "text": "Cohort 01\n\n\n\n\nLOG"
  },
  {
    "objectID": "how-to.html#set-up-quarto",
    "href": "how-to.html#set-up-quarto",
    "title": "How to add to the book",
    "section": "Set up Quarto",
    "text": "Set up Quarto\nThis book is made with Quarto. Please see the Get Started chapter of the Quarto documentation to learn how to install and run Quarto in your IDE."
  },
  {
    "objectID": "how-to.html#add-to-book",
    "href": "how-to.html#add-to-book",
    "title": "How to add to the book",
    "section": "Add to book",
    "text": "Add to book\nOnce you have everything set up, forked the repo, and cloned to your computer, you can add a new chapter to the book.\nCreate a new file in the repository folder. For example, to create a new file called 01_exercises.qmd, navigate to the folder then create one using touch 01_exercises.qmd. If you are using VSCode, you can use the Quarto plug-in. You can use plain .md files, Quarto .qmd, or Jupyter .ipynb files in this book. Check out the files under Examples to see the various options.\nWrite in what you would like in the file.\nThen, in the _quarto.yml file, under chapters, add a part with your chapter. The file listed after part is the first page of chapter; the ones under chapters will be subpages.\n  - part: 01_main.qmd\n      chapters: \n      - 01_notes.qmd\n      - 01_video.qmd\n      - 01_exercises.qmd"
  },
  {
    "objectID": "how-to.html#render-the-book",
    "href": "how-to.html#render-the-book",
    "title": "How to add to the book",
    "section": "Render the book",
    "text": "Render the book\nOnce you have added and edited your files, don’t forget to render the book. Run this in the terminal:\nquarto render --to html"
  },
  {
    "objectID": "how-to.html#push-up-to-github",
    "href": "how-to.html#push-up-to-github",
    "title": "How to add to the book",
    "section": "Push up to GitHub",
    "text": "Push up to GitHub\nPush your changes to your forked repo and then create a pull request for the R4DS admins to merge your changes.\ngit add .\ngit commit -m \"Message here\"\ngit push"
  }
]